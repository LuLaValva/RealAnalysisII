\documentclass{article}
% Control margins of the page
% \usepackage[margin=0.5in,top=1in]{geometry}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{titling}
\usepackage{scrextend}


\title{Real Analysis II - Homework I}
\author{Lucas LaValva}
\date{\today}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\theauthor}
\rhead{\thetitle}

\begin{document}
\maketitle

\setcounter{section}{4}

\section{Sequences of Functions}
\subsection{Pointwise and Uniform Convergence}
\begin{enumerate}
      \setcounter{enumi}{1}
      \item Suppose that $g$ is a continuous function on $[a,b]$.
            \begin{enumerate}
                  \item Prove that if $f_n\to f$ pointwise,
                        then $gf_n\to gf$ pointwise.
                        \begin{proof}
                              Because $f_n\to f$ pointwise, we know that,
                              given $\varepsilon > 0$ and $x\in [a,b]$,
                              there exists $N_0$ such that for all
                              $n>N_0$, $\lvert f_n(x)-f(x)\rvert<\varepsilon$.
                              Let $g$ be a continuous function on $[a,b]$, and
                              choose $N_1>N_0\times\max(\lvert g(x)\rvert)$. Then
                              \begin{align*}
                                    \lvert f_n(x)-f(x)\rvert                                         & < \varepsilon                        \\
                                    \lvert g(x)\rvert\lvert f_n(x)-f(x)\rvert                        & < \lvert g(x)\rvert\varepsilon       \\
                                    \lvert g(x)f_n(x)-g(x)f(x)\rvert                                 & < \max(\lvert g(x)\rvert)\varepsilon \\
                                    \frac{\lvert g(x)f_n(x)-g(x)f(x)\rvert}{\max(\lvert g(x)\rvert)} & < \varepsilon
                              \end{align*}
                              We know that $g$ is continuous on $[a,b]$, which implies that
                              $\max(g(x))$ is a real number less than infinity.
                              This shows that there exists $N_0\leq N_1\leq \infty$ such that for
                              all $n>N_1$, $\lvert g(x)f_n(x)-g(x)f(x)\rvert<\varepsilon$.
                        \end{proof}
                  \item Prove that if $f_n\to f$ uniformly,
                        then $gf_n\to gf$ uniformly.
                        \begin{proof}
                              Given $\varepsilon>0$, there exists an $N$ so that
                              $n>N$ implies $\lvert f_n(x)-f(x)\rvert < \varepsilon$
                              for all $x\in [a,b]$. Then
                              \begin{align*}
                                    \lvert f_n(x)-f(x)\rvert                                         & < \varepsilon                        \\
                                    \lvert g(x)\rvert\lvert f_n(x)-f(x)\rvert                        & < \lvert g(x)\rvert\varepsilon       \\
                                    \lvert g(x)f_n(x)-g(x)f(x)\rvert                                 & < \max(\lvert g(x)\rvert)\varepsilon \\
                                    \frac{\lvert g(x)f_n(x)-g(x)f(x)\rvert}{\max(\lvert g(x)\rvert)} & < \varepsilon
                              \end{align*}
                              We know that $g$ is continuous on $[a,b]$, which implies that
                              $\max(g(x))$ is a real number less than infinity.
                              This shows that there exists $N_0\leq N_1\leq \infty$ such that for
                              all $n>N_1$, $\lvert g(x)f_n(x)-g(x)f(x)\rvert<\varepsilon$.

                        \end{proof}
            \end{enumerate}
            \setcounter{enumi}{4}
      \item Prove that $f_n(x)=(x-\frac{1}{n})^2$ converges uniformly
            on any finite interval
            \begin{proof}
                  Let $f(x)=x^2$. Given $\varepsilon>0$, choose $N>\frac{2\max\{\lvert a\rvert, \lvert b\rvert\}+1}{\varepsilon}$. Then, for
                  all $x\in [a,b]$, $n>N$ implies
                  \begin{align*}
                        \lvert f_n(x)-f(x)\rvert & = \left\lvert \left(x-\frac{1}{n}\right)^2-x^2\right\rvert    \\
                                                 & = \left\lvert x^2-2x\frac{1}{n}+\frac{1}{n}^2-x^2\right\rvert \\
                                                 & = \left\lvert \frac{2x}{n}+\frac{1}{n^2}\right\rvert          \\
                                                 & \leq \frac{2\lvert x\rvert}{n}+\frac{1}{n^2}                  \\
                                                 & \leq \frac{2\lvert x\rvert}{n}+\frac{1}{n}                    \\
                                                 & \leq \frac{2\lvert x\rvert+1}{n}                              \\
                                                 & \leq \frac{2\lvert x\rvert+1}{N}                              \\
                                                 & \leq \frac{2\max\{\lvert a\rvert, \lvert b\rvert\}+1}{N}      \\
                                                 & \leq \varepsilon
                  \end{align*}
                  Thus $f_n$ converges uniformly on $[a,b]$.
            \end{proof}
            \setcounter{enumi}{6}
      \item Let $f_n(x) = \frac{nx}{1+n^2x^2}$. Prove that $f_n\to 0$
            pointwise but not uniformly on $[0,1]$.
            \begin{proof}
                  Let $f(x)=0$. Given $\varepsilon>0$ and $x\in[0,1]$,
                  choose $N>\frac{1}{x\varepsilon}$.
                  Then, for all $n>N$, we have the following if $x=0$:
                  \begin{align*}
                        \lvert f_n(x)-f(x)\rvert & = \left\lvert\frac{n(0)}{1+n^2(0)^2}-0\right\rvert \\
                                                 & = \left\lvert\frac{0}{1}\right\rvert               \\
                                                 & = \lvert0\rvert=0\leq\varepsilon
                  \end{align*}
                  and the following if $x\neq 0\implies x>0$:
                  \begin{align*}
                        \lvert f_n(x)-f(x)\rvert & = \left\lvert\frac{nx}{1+n^2x^2}-0\right\rvert \\
                                                 & < \frac{nx}{n^2x^2} = \frac{1}{nx}             \\
                                                 & < \frac{1}{Nx}                                 \\
                                                 & < \varepsilon
                  \end{align*}
                  Therefore, $f_n(x)$ converges pointwise. However, given any
                  $\varepsilon>0$, there exists some $x$ such that
                  $\frac{1}{Nx}\geq\varepsilon$. Therefore, $f_n(x)$ does not
                  converge uniformly.
            \end{proof}
            \setcounter{enumi}{13}
      \item \begin{enumerate}
                  \item Prove that $(1+\frac{x}{n})^n\to e^x$
                        for all $x$.
                        \begin{proof}
                              \begin{align*}
                                      & \left\lvert(1+\frac{x}{n})^n-e^x\right\rvert \\
                                    = &                                              % TODO
                              \end{align*}
                        \end{proof}
                  \item Prove that $(1+\frac{x}{n})^n\to e^x$
                        uniformly on any finite interval $[a,b]$.
                        \begin{proof}
                              % TODO
                        \end{proof}
                  \item Prove that the convergence is not
                        uniform on $\mathbb{R}$.
                        \begin{proof}
                              % TODO
                        \end{proof}
            \end{enumerate}
\end{enumerate}

\subsection{Limit Theorems}
\begin{enumerate}
      \setcounter{enumi}{1}
      \item Compute the following limits:
            \begin{enumerate}
                  \item $\lim_{n\to \infty}\int_0^1(x+\frac{1}{n})^2dx$.
                        \begin{align*}
                                & \lim_{n\to \infty}\int_0^1(x+\frac{1}{n})^2dx \\
                              \intertext{$(x+\frac{1}{n})^2$ converges uniformly on $[0,1]$, so we can bring the limit in.}
                              = & \int_0^1\lim_{n\to \infty}(x+\frac{1}{n})^2dx \\
                              = & \int_0^1x^2dx                                 \\
                              = & \left.\frac{x^3}{3}\right\rvert_0^1           \\
                              = & \frac{1^3}{3}-\frac{0^3}{3}                   \\
                              = & \frac{1}{3}
                        \end{align*}
                  \item $\lim_{n\to \infty}\int_1^2e^{-nx}dx$.
                        \begin{align*}
                                & \lim_{n\to \infty}\int_1^2e^{-nx}dx \\
                              \intertext{$e^{-nx}$ converges uniformly on $[1,2]$, so we can bring the limit in.}
                              = & \int_1^2\lim_{n\to \infty}e^{-nx}dx \\
                              = & \int_1^20dx                         \\
                              = & 0
                        \end{align*}
                  \item $\lim_{n\to \infty}\int_0^{\frac{\pi}{2}}
                              \sin (x+\frac{1}{n})dx$.
                        \begin{align*}
                                & \lim_{n\to \infty}\int_0^{\frac{\pi}{2}}\sin (x+\frac{1}{n})dx \\
                              \intertext{$\sin (x+\frac{1}{n})$ converges uniformly on $[0,{\frac{\pi}{2}}]$, so we can bring the limit in.}
                              = & \int_0^{\frac{\pi}{2}}\lim_{n\to \infty}\sin (x+\frac{1}{n})dx \\
                              = & \int_0^{\frac{\pi}{2}}\sin(x)dx                                \\
                              = & \left.-\cos(x)\right\rvert_0^{\frac{\pi}{2}}                   \\
                              = & -\cos\left({\frac{\pi}{2}}\right)+\cos(0)                      \\
                              = & 0 + 1 = 1
                        \end{align*}
                  \item $\lim_{n\to \infty}\int_0^1(1+\frac{x}{n})^ndx$.
                        \begin{align*}
                                & \lim_{n\to \infty}\int_0^1(1+\frac{x}{n})^ndx \\
                              \intertext{$(1+\frac{x}{n})^n$ converges uniformly on $[0,1]$, so we can bring the limit in.}
                              = & \int_0^1\lim_{n\to \infty}(1+\frac{x}{n})^ndx \\
                              = & \int_0^1e^xdx                                 \\
                              = & \left.e^x\right\rvert_0^1                     \\
                              = & e^1-e^0                                       \\
                              = & e-1 \approx 1.732
                        \end{align*}
            \end{enumerate}
            \setcounter{enumi}{3}
      \item Let $\{f_n\}$ be a sequence of continuous functions on a finite
            interval $[a,b]$ that converges uniformly to $f$. Show that for
            all continuous functions $g$ on $[a,b]$,
            \[
                  \lim_{x \to \infty}\int_a^bf_n(t)g(t)dt = \int_a^bf(t)g(t)dt.
            \]
            \begin{proof}
                  Let $\varepsilon>0$ be given. Since $f_n\to f$ uniformly, we
                  know that there exists some $N$ so that for all $n>N$,
                  \[
                        \lvert f_n-f\rvert < \frac{\varepsilon}{\int_a^b\lvert g(x)\rvert dx} \text{for all }x\in [a,b].
                  \]
                  We know that, because $g$ is a continuous function,
                  $\int_a^b\lvert g(x)\rvert dx$ exists for all $a,b\in \mathbb{R}$. Thus,
                  \begin{align*}
                        \left\lvert\int_a^b(f_n(x)g(x))dx-\int_a^b(f(x)g(x))dx\right\rvert & = \left\lvert\int_a^b(f_n(x)g(x)-f(x)g(x))dx\right\rvert                       \\
                                                                                           & \leq \int_a^b\lvert f_n(x)g(x)-f(x)g(x)\rvert dx                               \\
                                                                                           & = \int_a^b\lvert g(x)\rvert\lvert f_n(x)-f(x)\rvert dx                         \\
                                                                                           & < \int_a^b\lvert g(x)\rvert\frac{\varepsilon}{\int_a^b\lvert g(x)\rvert dx} dx \\
                                                                                           & = \frac{\varepsilon}{\int_a^b\lvert g(x)\rvert dx}\int_a^b\lvert g(x)\rvert dx \\
                                                                                           & =\varepsilon
                  \end{align*}
                  Therefore, $\int_a^bf_n(t)g(t)dt\to \int_a^bf(t)g(t)dt$.
            \end{proof}
            \setcounter{enumi}{6}
      \item Suppose that $\{f_n\}$ is a sequence of continuous functions on an
            open interval $(a,b)$ that converges uniformly to $f$ on $(a,b)$.
            Suppose that each $f_n$ is uniformly continuous on $(a,b)$. Prove
            that $f$ is uniformly continuous on $(a,b)$.
            \begin{proof}
                  Let $\varepsilon>0$ be given and $x_o\in(a,b)$. Since $f_n\to f$ uniformly, we
                  know there exists some $N$ so that for all $n>N$,
                  \begin{equation}
                        \lvert f_n-f\rvert < \frac{\varepsilon}{3}
                        \label{eq:5.2_7_uniformConvergence}
                  \end{equation}
                  Since $f_N$ is continuous, we know that there exists some
                  $\delta$ so that
                  \begin{equation}
                        \lvert f_N(x)-f_N(x_o)\rvert < \frac{\varepsilon}{3}
                        \text{ if } \lvert x-x_o\rvert < \delta.
                        \label{eq:5.2_7_continuous}
                  \end{equation}
                  Using \eqref{eq:5.2_7_uniformConvergence} and
                  \eqref{eq:5.2_7_continuous}, we can show that
                  \begin{align*}
                        \lvert f(x)-f(x_o)\rvert & = \lvert f_N(x)-f(x)+f_N(x)-f_N(x_o)+f_N(x_o)-f(x_o)\rvert                              \\
                                                 & \leq \lvert f_N(x)-f(x)\rvert+\lvert f_N(x)-f_N(x_o)\lvert+\rvert f_N(x_o)-f(x_o)\rvert \\
                                                 & < \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3}                 \\
                                                 & = \varepsilon.
                  \end{align*}
                  Therefore, $f$ must be continuous since $\lvert f(x)-f(x_o)\rvert<\varepsilon$.
            \end{proof}
            \setcounter{enumi}{14}
      \item Suppose that ${f_n}$ is a sequence of continuous functions on a
            finite interval $[a,b]$. Suppose that $f_n\to 0$ pointwise
            on $[a,b]$ and that for each $x\epsilon [a,b]$ the sequence of
            numbers $\{f_n(x)\}$ is nonincreasing. Prove that $f_n\to 0$
            uniformly.
            \begin{proof}
                  Let it be known that $\varepsilon>0$. Since $f_n\to 0$ pointwise,
                  we know there exists some $N$ for each $x\in[a,b]$ so that for
                  all $n>N$
                  \begin{equation}
                        \lvert f_n(x)-0\rvert<\varepsilon.
                        \label{eq:5.2_15_pointwiseConvergence}
                  \end{equation}
                  We also know that, since $f_n$ is nonincreasing at each $x_o$,
                  \begin{equation}
                        f_{n+1}(x_o) \leq f_n(x_o).
                        \label{eq:5.2_15_nonincreasing}
                  \end{equation}
                  Using \eqref{eq:5.2_15_pointwiseConvergence} and
                  \eqref{eq:5.2_15_nonincreasing}, we can show that
                  \begin{align*}
                        \lvert f_n(x)-0\rvert & = \lvert f_n(x)\rvert    \\
                                              & \leq \lvert f_n(x)\rvert
                  \end{align*}% TODO
            \end{proof}
\end{enumerate}

\subsection{The Supremum Norm}
\begin{enumerate}
      \setcounter{enumi}{1}
      \item Let $f$ and $g$ be continuous functions on $[a,b]$.
            \begin{enumerate}
                  \item Use the triangle inequality to prove that
                        \[
                              \lvert
                              \lVert f\rVert_\infty - \lVert g\rVert_\infty
                              \rvert \leq \lVert f-g\rVert_\infty
                        \]
                        \begin{proof}
                              We know that
                              \begin{align*}
                                    \lVert f\rVert_\infty                         & = \lVert f-g+g\rVert_\infty                          \\
                                    \lVert f\rVert_\infty                         & \leq \lVert f-g\rVert_\infty + \lVert g\rVert_\infty \\
                                    \lVert f\rVert_\infty - \lVert g\rVert_\infty & \leq \lVert f-g\rVert_\infty                         \\
                                    \intertext{and}
                                    \lVert g\rVert_\infty                         & = \lVert g-f+f\rVert_\infty                          \\
                                    \lVert g\rVert_\infty                         & \leq \lVert g-f\rVert_\infty + \lVert f\rVert_\infty \\
                                    \lVert g\rVert_\infty - \lVert f\rVert_\infty & \leq \lVert g-f\rVert_\infty.                        \\
                              \end{align*}
                              Therefore, because we know that $\lVert f\rVert_\infty - \lVert g\rVert_\infty$
                              and $\lVert g\rVert_\infty - \lVert f\rVert_\infty$ are both less than $\lVert f-g\rVert_\infty$,
                              \[
                                    \lvert
                                    \lVert f\rVert_\infty - \lVert g\rVert_\infty
                                    \rvert \leq \lVert f-g\rVert_\infty.
                              \]
                        \end{proof}
                  \item Suppose that $f_n\to f$ in the sup norm. Prove
                        that $\lVert f_n\rVert_\infty\to\lVert f\rVert_\infty$.
                        \begin{proof}
                              Let $\varepsilon>0$ be given.
                              We know that $f_n\to f$ in the sup norm, so there
                              must exist some $N$ such that for all $n>N$,
                              \begin{equation}
                                    \lVert f_n-f\rVert_\infty<\varepsilon.
                                    \label{eq:5.3_2b_convergence}
                              \end{equation}
                              Using \eqref{eq:5.3_2b_convergence}, and the proposition
                              from part (a), it is clear that
                              \begin{align*}
                                    \lvert\lVert f_n\rVert_\infty-\lVert f\rVert_\infty\rvert & \leq \lVert f_n-f\rVert_\infty \\
                                                                                              & < \varepsilon.
                              \end{align*}
                              Therefore, $\lVert f_n\rVert_\infty\to\lVert f\rVert_\infty$.

                        \end{proof}
            \end{enumerate}
      \item Let $C_b(\mathbb{R})$ denote the set of bounded continuous functions
            on $\mathbb{R}$. Prove that $C_b(\mathbb{R})$ is complete in the sup
            norm.
            \begin{proof}
                  Let $f_o$ be any arbitrary function in $C_b(\mathbb{R})$. Then
                  because $f_o$ it is bounded, there must exist some $y$ such that
                  for all $x$, $f_o(x)<y$. Therefore, $\lVert f_o\rVert_\infty$
                  exists for all functions $f_o$ in $C_b(\mathbb{R})$.
            \end{proof}
            \setcounter{enumi}{4}
      \item Let the functions $f_n$ be defined on $[0,1]$ by
            \begin{equation}
                  f_n(x)=\begin{cases}
                        1
                         & 0\leq x \leq \frac{1}{2}                     \\

                        1-n(x-\frac{1}{2})
                         & \frac{1}{2} < x \leq \frac{1}{2}+\frac{1}{n} \\

                        0 % 
                         & \frac{1}{2}+\frac{1}{n} < x \leq 1
                  \end{cases}
            \end{equation}
            and define
            \begin{equation}
                  f(x)=\begin{cases}
                        1 & 0 \leq x \leq \frac{1}{2} \\
                        0 & \frac{1}{2} < x \leq 1.
                  \end{cases}
            \end{equation}
            \begin{enumerate}
                  \item Prove that $f_n\to f$ pointwise on $[0,1]$.
                        \begin{proof}
                              Let $\varepsilon>0$ be given and $N>\frac{1}{\varepsilon}$. Then, for
                              all $n>N$, the following two cases can be
                              shown: \\
                              Case 1 — $0\leq x \leq \frac{1}{2}\implies$
                              \begin{align*}
                                    f_n(x)             & = 1    \\
                                    f(x)               & = 1    \\
                                    \therefore  f_n(x) & = f(x)
                              \end{align*}
                              Case 2 — $\frac{1}{2}< x \leq 1.$ \\
                              Let us assume that $\frac{1}{2}< x \leq\frac{1}{2} + \frac{1}{n}$.
                              Then
                              \begin{align*}
                                    \lvert\frac{1}{n}-0\rvert & = \lvert\frac{1}{n}\rvert \\
                                                              & = \frac{1}{n}             \\
                                                              & < \frac{1}{N}             \\
                                                              & < \varepsilon.
                              \end{align*}
                              Therefore,
                              \[
                                    \frac{1}{2}< x \leq\frac{1}{2}
                              \]
                              This clearly cannot be true. Contraction!
                              Thus,
                              \begin{align*}
                                    \frac{1}{2} + \frac{1}{n}< & x \leq 1 \\
                                    f_n(x)                     & = 0      \\
                                    f(x)                       & = 0      \\
                                    \therefore f_n(x)          & = f(x)
                              \end{align*}
                              Therefore, $f_n(x)\to f(x)$ pointwise.
                        \end{proof}
                  \item Prove that $\lVert f-f_n\rVert_\infty=1$ for each $n$ so
                        that $f_n$ does not converge to $f$ in the $sup$ norm.
                        \begin{proof}
                              % TODO
                        \end{proof}
                  \item Explain how you could have predicted the result of part
                        (b) simply by using theorem 5.2.1.
                        \begin{description}
                              \item[Theorem 5.2.1] Let $\{f_n\}$ be a continuous
                                    set of functions on a set $E\subseteq\mathbb{R}$.
                                    If $f_n\to f$ uniformly on $E$ as $n\to\infty$ then
                                    $f$ is continuous on $E$.
                        \end{description}
                        % TODO
                  \item Prove that $\lVert f-f_n\rVert_1\to 0$ as $n\to \infty$.
                        \begin{proof}
                              Let $\varepsilon>0$ be given, and choose $N>$.
                              Then, for all $n>N$, % TODO
                        \end{proof}
            \end{enumerate}
            \setcounter{enumi}{6}
      \item Let $C^{(1)}[a,b]$ denote the set of continuously differentiable
            functions on a finite interval $[a,b]$. For each $f$ in
            $C^{(1)}[a,b]$, define $\lVert f\rVert \equiv \lVert f\rVert_\infty
                  + \lVert f'\rVert_\infty$.
            \begin{enumerate}
                  \item Show that $\lVert f\rVert$ has the properties (a), (b),
                        and (c) of Proposition 5.3.1.
                        \begin{description}
                              \item[(a)] $\lVert f\rVert_\infty\geq 0$ and
                                    $\lVert f\rVert_\infty=0$ if and only if $f$
                                    is the zero function on $E$.
                                    \begin{proof}
                                          Let $f$ be any arbitrary function on
                                          $C^{(1)}[a,b]$. Because $C^{(1)}[a,b]$
                                          is continuously differentiable we know
                                          that $f$ and $f'$ are both continuous,
                                          and thus able to be measured by the sup
                                          norm. Therefore,
                                          \begin{align*}
                                                (\lVert f\rVert_\infty \geq 0) & \land (\lVert f'\rVert_\infty  \geq 0) & \implies\lVert f\rVert_\infty + \lVert f'\rVert_\infty & \geq 0 \\
                                                (\lVert f\rVert_\infty = 0)    & \land (\lVert f'\rVert_\infty  = 0)    & \implies\lVert f\rVert_\infty + \lVert f'\rVert_\infty & = 0    \\
                                                (\lVert f\rVert_\infty > 0)    & \lor (\lVert f'\rVert_\infty  > 0)     & \implies\lVert f\rVert_\infty + \lVert f'\rVert_\infty & > 0
                                          \end{align*}
                                    \end{proof}
                              \item[(b)] For every $\alpha\in\mathbb{R}$, we have
                                    $\lVert\alpha f\rVert_\infty=\lvert\alpha\rvert\lVert f\rVert_\infty$.
                                    \begin{proof}
                                          \begin{align*}
                                                \lvert\alpha\rvert\lVert f\rVert & = \lvert\alpha\rvert(\lVert f\rVert_\infty+\lVert f'\rVert_\infty)                 \\
                                                                                 & = \lvert\alpha\rvert\lVert f\rVert_\infty+\lvert\alpha\rvert\lVert f'\rVert_\infty \\
                                                                                 & = \lVert\alpha f\rVert_\infty+\lVert\alpha f'\rVert_\infty                         \\
                                                                                 & = \lVert\alpha f\rVert
                                          \end{align*}
                                    \end{proof}
                              \item[(c)] $\lVert f+g\rVert_\infty\leq\lVert f\rVert_\infty+\lVert g\rVert_\infty$
                                    (the triangle inequality).
                                    \begin{proof}
                                          \begin{align*}
                                                \lVert f+g\rVert & = \lVert f+g\rVert_\infty+\lVert (f+g)'\rVert_\infty                                                  \\
                                                                 & = \lVert f+g\rVert_\infty+\lVert f'+g'\rVert_\infty                                                   \\
                                                                 & = \lVert f+g\rVert_\infty+\lVert f'+g'\rVert_\infty                                                   \\
                                                                 & \leq \lVert f\rVert_\infty + \lVert g\rVert_\infty + \lVert f'\rVert_\infty + \lVert g'\rVert_\infty  \\
                                                                 & = (\lVert f\rVert_\infty + \lVert f'\rVert_\infty) + (\lVert g\rVert_\infty + \lVert g'\rVert_\infty) \\
                                                                 & = \lVert f\rVert + \lVert g\rVert
                                          \end{align*}
                                    \end{proof}
                        \end{description}
                  \item Prove that $C^{(1)}[a,b]$ is complete in the norm
                        $\lVert f\rVert$.
                        \begin{proof}
                              Let $\{f_n\}$ be a Cauchy sequence in the norm
                              $\lVert f\rVert$, and $\varepsilon>0$ be given.
                              Because $\{f_n\}$ is Cauchy, there exists some $N$
                              such that, for all $n,m>N$,
                              \begin{equation}
                                    \lVert f_n-f_m\rVert<\varepsilon.
                              \end{equation}
                              Thus, for each $x\in\mathbb{R}$, since
                              \begin{equation}
                                    \lvert f_n(x)-f_m(x)\rvert \leq \lVert f_n(x)-f_m(x)\rVert
                              \end{equation}
                              we know that
                              \begin{equation}
                                    \lvert f_n(x)-f_m(x)\rvert < \varepsilon
                                    \label{eq:5.3_7b_abs}
                              \end{equation}
                              Define $f(x)\equiv \lim_{n\to\infty}f_n(x)$. The
                              absolute value is a continuous function, so
                              \begin{equation*}
                                    \lvert f(x)-f_m(x)\rvert = \lim_{n\to\infty}\lvert f_n(x)-f_m(x)\rvert.
                              \end{equation*}
                              Then, \eqref{eq:5.3_7b_abs} implies that for all
                              $m>N$ and $x\in[a,b]$
                              \[
                                    \lvert f(x)-f_m(x)\rvert \leq \varepsilon
                              \]
                              This shows that $f_m\to f$ uniformly. Therefore,
                              $f$ is continuous and $f_n$ converges to $f$ in
                              the norm $\lVert f\rVert$.
                        \end{proof}
            \end{enumerate}
\end{enumerate}

\setcounter{subsection}{4}
\subsection{The Calculus of Variations}
\begin{enumerate}
      \item Suppose that $H(x)$ is a continuous function on the interval
            $[x_1, x_2]$ and that
            \begin{equation}
                  \int_{x_1}^{x_2}H(x)\eta (x)dx = 0
                  \label{eq:5.5_e1}
            \end{equation}
            for all twice continuously differentiable functions $\eta (x)$ that
            vanish at the endpoints. Prove that $H(x)\equiv 0$ in the interval
            as follows:
            \begin{enumerate}
                  \item Let $[a,b]$ be any finite interval. Show how to
                        construct a twice continuously differentiable function
                        on $\mathbb{R}$ which is strictly positive on the open
                        interval $(a,b)$ and identically zero everywhere else.
                        \begin{align*}
                              f(x)   & =\begin{cases}
                                    0               & \text{if } x \leq a  \\
                                    -(x-a)^3(x-b)^3 & \text{if } a < x < b \\
                                    0               & \text{if } b \leq x
                              \end{cases}  \\
                              f'(x)  & =\begin{cases}
                                    0                                  & \text{if } x \leq a  \\
                                    -3(x-a)^3(x-b)^2 - 3(x-a)^2(x-b)^3 & \text{if } a < x < b \\
                                    0                                  & \text{if } b \leq x
                              \end{cases}  \\
                              f''(x) & =\begin{cases}
                                    0                          & \text{if } x \leq a \\
                                    \begin{matrix}
                                          -6(x-a)(x-b)^3 - 18(x-a)^2(x-b)^2 \\
                                          - 6(x-a)^3(x-b)
                                    \end{matrix} &
                                    \text{if } a < x < b                             \\
                                    0                          & \text{if } b \leq x
                              \end{cases} \\
                        \end{align*}
                  \item If $x_o$ is a point of $[x_1, x_2]$ such that $H(x_o)
                              \neq 0$, show how to choose $\eta (x)$ so that
                        hypothesis \eqref{eq:5.5_e1} is violated.
                        \begin{proof}
                              Any $\eta$ that contains no zeroes in
                              $(a,b)$ will violate \eqref{eq:5.5_e1} if there
                              exists some $x_o$ in which $H(x_o)\neq 0$. This
                              is because $H(x_o)\eta(x_o)\neq 0$, which implies
                              that the integral cannot be 0. % TODO: CAN BE IMPROVED
                        \end{proof}
            \end{enumerate}
            \setcounter{enumi}{2}
      \item Find a curve passing through $(1,2)$ and $(2,4)$ that is an extremal
            for the functional
            \begin{equation}
                  J(x, y') = \int_1^2xy'(x)+(y'(x))^2dx.
            \end{equation}
            \begin{proof}
                  Let $f(x, y, y')=xy'+(y')^2$. Using the Euler-Lagrange equation,
                  \begin{align*}
                        f_y(x, y') - \frac{d}{dx}f_{y'}(x, y')
                         & = 0 - \frac{d}{dx}(x+2y') \\
                         & = -(1+2y'')               \\
                         & = -2y'' - 1.
                  \end{align*}
                  Therefore,
                  \begin{align*}
                        -2y'' - 1 & = 0                        \\
                        y''       & = -\frac{1}{2}             \\
                        y'        & = -\frac{x}{2} + c_1       \\
                        y         & = -\frac{x^2}{4}+xc_1+c_2.
                  \end{align*}
                  To find the values of the constants, we are left with the following
                  system of equations:
                  \begin{align*}
                        2           & = -\frac{1^2}{4}+(1)c_1+c_2                \\
                        4           & = -\frac{2^2}{4}+(2)c_1+c_2                \\
                        \\
                        \frac{9}{4} & = c_1+c_2                                  \\
                        5           & = 2c_1+c_2                                 \\
                        \\
                        c_1         & = \frac{11}{4}                             \\
                        c_2         & = -\frac{1}{2}                             \\
                        \\
                        \therefore
                        y(x)        & = -\frac{x^2}{4}+\frac{11x}{4}-\frac{1}{2}
                  \end{align*}
            \end{proof}
            \setcounter{enumi}{4}
      \item Find a curve passing through $(0,0)$ and $(1,1)$ that is an extremal
            for the functional
            \begin{equation}
                  J(x, y, y') = \int_0^1(y'(x))^2+12xy(x)dx.
            \end{equation}
            \begin{proof}
                  Let $f(x, y, y')=(y')^2+12xy$. Using the Euler-Lagrange equation,
                  \begin{align*}
                        f_y(x, y') - \frac{d}{dx}f_{y'}(x, y')
                         & = 12x - \frac{d}{dx}2y' \\
                         & = 12x - 2y''.
                  \end{align*}
                  Therefore,
                  \begin{align*}
                        12x - 2y'' & = 0                 \\
                        y''        & = 6x                \\
                        y'         & = 3x^2 + c_1        \\
                        y          & = x^3 + xc_1 + c_2.
                  \end{align*}
                  We are left with the following system of equations:
                  \begin{align*}
                        0    & = (0)^3 + (0)c_1 + c_2 \\
                        1    & = (1)^3 + (1)c_1 + c_2 \\
                        \\
                        0    & = c_2                  \\
                        1    & = 1 + c_1 + c_2        \\
                        \\
                        c_1  & = 0                    \\
                        c_2  & = 0                    \\
                        \\
                        \therefore
                        y(x) & = x^3
                  \end{align*}
            \end{proof}
            \setcounter{enumi}{7}
      \item Let $y(x)$ be a twice differentiable function whose graph passes
            through the points $(x_1,y_1)$ and $(x_2,y_2)$ in the plane, where
            $y_1>0$ and $y_2>0$.
            \begin{enumerate}
                  \item Show that the surface area generated when the curve is
                        revolved around the $x$-axis is given by
                        \begin{equation}
                              J(y, y') = 2\pi\int_{x_1}^{x_2}y(x)
                              \sqrt{1+(y'(x))^2}dx.
                        \end{equation}
                        \begin{proof}
                              For any given $x_a$, $x_b$, where $x_1\leq x_a<x_b\leq x_2$,
                              the there exists some frustrum of a cone generated when the segment
                              between the points at $(x_a, y(x_a))$ and $(x_b, y(x_b))$ is revolved
                              around the $x$-axis. To find the surface area of this frustrum,
                              we can use
                              \begin{align*}
                                    A & = \frac{(\textrm{circumference of $x_a$'s circle}) + (\textrm{circumference of $x_b$'s circle})}{2} \\
                                      & \phantom{=} \times (\textrm{length of diagonal})                                                    \\
                                      & = \frac{2\pi y(x_a) + 2\pi y(x_b)}{2}\sqrt{(x_a-x_b)^2+((y(x_a)-y(x_b))^2)}                         \\
                                      & = \pi(y(x_a)+y(x_b))\sqrt{\Delta x^2+\Delta y^2}                                                    \\
                                      & = \pi(y(x_a)+y(x_b))\sqrt{\Delta x^2+(\frac{\Delta x\Delta y}{\Delta x})^2}                         \\
                                      & = \pi(y(x_a)+y(x_b))\sqrt{\Delta x^2+\Delta x^2(\frac{\Delta y}{\Delta x})^2}                       \\
                                      & = \pi(y(x_a)+y(x_b))\sqrt{1+(\frac{\Delta y}{\Delta x})^2}\Delta x.
                              \end{align*}
                              As $x_a\to x_b$, $A\to 2\pi y(x)\sqrt{1+(\frac{dy}{dx})^2}dx$. Then we can denote
                              the surface area of this integral as
                              \[
                                    2\pi\int_{x_1}^{x_2}y(x)\sqrt{1+(y'(x))^2}dx.
                              \]
                        \end{proof}
                  \item Show that if $y(x)$ is an extremal, then $y(x)$
                        satisfies
                        \[
                              \frac{y(x)}{\sqrt{1+(y'(x))^2}}=C_1.
                        \]
                        \begin{proof}
                              If $y(x)$ is an extremal, then there exists some
                              $f(x,y,y')$ such that
                              \[
                                    \int_{x_1}^{x_2} f(x,y(x),y'(x))dx
                              \]
                              has a local maximum or minimum at $y(x)$. Then,
                              by the Euler-Lagrange equation,
                              \[
                                    f_y(x,y,y') - \frac{d}{dx}f_{y'}(x,y,y')=0.
                              \]
                              % TODO: I truly don't understand this one
                        \end{proof}
                  \item Show that the functions $y(x)=C_2\cosh
                              (\frac{x-C_1}{C_2})$ satisfy the differential
                        equation for all choices of $C_1$ and $C_2$.
                  \item Show that $C_1$ and $C_2$ can be chosen so that the
                        graph of $y$ passes through $(x_1, y_1)$ and
                        $(x_2, y_2)$.
            \end{enumerate}
\end{enumerate}

\subsection{Metric Spaces}
\begin{enumerate}
      \item Prove that the functions $\rho_1$ and $\rho_{\max}$ defined in
            example 3 are indeed metrics on $\mathbb{R}^2$.
            \begin{align*}
                  \rho_1((x_1, y_1), (x_2, y_2))      & \equiv \lvert x_1-x_2\rvert + \lvert y_1-y_2\rvert        \\
                  \rho_{\max}((x_1, y_1), (x_2, y_2)) & \equiv \max\{\lvert x_1-x_2\rvert, \lvert y_1-y_2\rvert\}
            \end{align*}
            \begin{proof}
                  To prove that $\rho_1$ is a metric, we must show that it satisfies
                  the following criteria:
                  \begin{enumerate}
                        \item $\forall (x_1,y_1),(x_2,y_2)\in\mathbb{R}^2, \rho_1((x_1,y_1),(x_2,y_2))\geq 0$
                              \medbreak
                              By definition, $\lvert x_1-x_2\rvert\geq 0$ and $\lvert y_1-y_2\rvert\geq 0$.
                              Therefore, $\lvert x_1-x_2\rvert+\lvert y_1-y_2\rvert\geq 0$
                              \medbreak
                              $\rho_1=0$ iff $(x_1,y_1)=(x_2,y_2)$.
                              \medbreak
                              Let us assume that $(x_1,y_1)\neq (x_2,y_2)$. Then at least one of $\lvert x_1-x_2\rvert\neq 0$
                              $\lvert y_1-y_2\rvert\neq 0$ is true. Therefore, $\lvert x_1-x_2\rvert+\lvert y_1-y_2\rvert> 0$.
                              However, if $(x_1,y_1) = (x_2,y_2)$, then $\lvert x_1-x_2\rvert = \lvert y_1-y_2\rvert = 0$ and
                              therefore $\rho_1((x_1,y_1),(x_2,y_2))=0$

                        \item $\rho_1((x_1,y_1),(x_2,y_2)) = \rho_1((x_2,y_2),(x_1,y_1))$
                              \medbreak
                              By the commutative property of addition,
                              $\lvert x_1-x_2\rvert+\lvert y_1-y_2\rvert=\lvert y_1-y_2\rvert+\lvert x_1-x_2\rvert$

                        \item $\forall(x_1,y_1),(x_2,y_2),(x_3,y_3)\in\mathbb{R}^2,$ \\
                              $\rho_1((x_1,y_1),(x_3,y_3))\leq \rho_1((x_1,y_1),(x_2,y_2))+\rho_1((x_2,y_2),(x_3,y_3))$
                              \begin{align*}
                                    \rho_1((x_1, y_1),(x_3,y_3)) & = \lvert x_1-x_3\rvert+\lvert y_1-y_3\rvert                                              \\
                                                                 & = \lvert x_1-x_2+x_2-x_3\rvert+\lvert y_1-y_2+y_2-y_3\rvert                              \\
                                                                 & \leq \lvert x_1-x_2\rvert+\lvert x_2-x_3\rvert+\lvert y_1-y_2\rvert+\lvert y_2-y_3\rvert \\
                                                                 & = \rho_1((x_1,y_1),(x_2,y_2))+\rho_1((x_2,y_2),(x_3,y_3))
                              \end{align*}
                  \end{enumerate}
            \end{proof}
            \begin{proof}
                  To prove that $\rho_{\max}$ is a metric, we must show that it satisfies
                  the following criteria:
                  \begin{enumerate}
                        \item $\forall (x_1,y_1),(x_2,y_2)\in\mathbb{R}^2, \rho_{\max}((x_1,y_1),(x_2,y_2))\geq 0$
                              \medbreak
                              By definition, $\lvert x_1-x_2\rvert\geq 0$ and $\lvert y_1-y_2\rvert\geq 0$.
                              Therefore, $\max(\lvert x_1-x_2\rvert,\lvert y_1-y_2\rvert)\geq 0$
                              \medbreak
                              $\rho_{\max}=0$ iff $(x_1,y_1)=(x_2,y_2)$
                              \medbreak
                              Let us assume that $(x_1,y_1)\neq (x_2,y_2)$. Then at least one of $\lvert x_1-x_2\rvert\neq 0$
                              $\lvert y_1-y_2\rvert\neq 0$ is true. Therefore, $\max(\lvert x_1-x_2\rvert,\lvert y_1-y_2\rvert)> 0$.
                              However, if $(x_1,y_1) = (x_2,y_2)$, then $\lvert x_1-x_2\rvert = \lvert y_1-y_2\rvert = 0$ and
                              therefore $\rho_{\max}((x_1,y_1),(x_2,y_2))=0$
                        \item $\rho_{\max}((x_1,y_1),(x_2,y_2)) = \rho_{\max}((x_2,y_2),(x_1,y_1))$
                              \medbreak
                              By definition, $\max$ is commutative and therefore $\rho_{max}$ is commutative.
                        \item $\forall(x_1,y_1),(x_2,y_2),(x_3,y_3)\in\mathbb{R}^2,\rho_{\max}((x_1,y_1),(x_3,y_3))$\\
                              \phantom{tabbing}$\leq \rho_{\max}((x_1,y_1),(x_2,y_2))+\rho_{\max}((x_2,y_2),(x_3,y_3))$
                              \begin{align*}
                                    \rho_{\max}(x_1,y_1), (x_3, y_3)) & = \max\{\lvert x_1-x_3\rvert,\lvert y_1-y_3\rvert\} \\
                                                                      & =                                                   \\% TODO: I never figured this one out
                              \end{align*}
                  \end{enumerate}
            \end{proof}
            \setcounter{enumi}{9}
      \item A metric space $(\mathcal{M}, \rho)$ is said to be \textit{discrete}
            if for every $x\epsilon\mathcal{M}$ there is an $\varepsilon>0$ so
            that $\rho(x, y)<\varepsilon$ implies $y=x$.
            \begin{enumerate}
                  \item Define a function $\delta$ on $\mathbb{R}$ by
                        $\delta(x,x)=0$ and $\delta(x, y)=1$ if $x\neq y$. Prove
                        that $(\mathbb{R}, \delta)$ is discrete.
                        \medbreak
                        $\delta(a,b)=\begin{cases}
                                    0 & \textrm{if } a = b    \\
                                    1 & \textrm{if } a \neq b
                              \end{cases}$
                        \begin{proof}
                              Let $\varepsilon>0$. By definition, if $x\neq y$ then
                              $\delta(x,y)=1$. Therefore, if $\varepsilon\leq 1$,
                              $x\neq y\implies \delta(x,y)\geq\varepsilon$.
                              Thus, the transposition is true:
                              $\delta(x,y)<\varepsilon\implies x=y$ for all $\varepsilon<1$.
                        \end{proof}
                  \item Prove that $(\mathbb{R}, \rho_2)$ is not discrete.
                        \begin{equation}
                              (\mathbb{R}^n,\rho_2(x,y)) = \sqrt{\sum_{i=1}^n\lvert x_i-y_i\rvert^2}
                        \end{equation}
                        \begin{proof}
                              On $\mathbb{R}^1$,
                              \begin{align*}
                                    \rho_2(x,y) & = \sqrt{\lvert x-y\rvert^2} \\
                                                & = \lvert x-y\rvert
                              \end{align*}
                              Let $\varepsilon>0$, and $x\in\mathbb{R}$. Then
                              \begin{align*}
                                    \rho_2(x, x+\frac{\varepsilon}{2}) & = \left\lvert x-(x+\frac{\varepsilon}{2})\right\rvert \\
                                                                       & = \left\lvert -\frac{\varepsilon}{2}\right\rvert      \\
                                                                       & = \frac{\varepsilon}{2}                               \\
                                                                       & < \varepsilon
                              \end{align*}
                              Because $\varepsilon>0$, $x+\frac{\varepsilon}{2}\neq x$. Therefore, $\rho_2$
                              is not discrete.
                        \end{proof}
                  \item Which of the metrics in Examples $1-5$ are discrete?
                        \medbreak
                        Most metrics in these examples are not discrete. The only one I found
                        was the one in example 5, where $\delta(q, p)$ is essentially normalized hamming
                        distance:
                        \begin{equation}
                              \rho(x,y) \equiv \sum_{i=1}^N\delta(q_i, p_i)
                        \end{equation}
                  \item In a discrete metric space $(\mathcal{M}, \rho)$, what
                        are the convergent sequences?
                        \medbreak
                        The only convergent sequences in discrete metrics are those
                        that are constant under the metric space.
            \end{enumerate}
            \setcounter{enumi}{11}
      \item Prove that the metrics $\rho_1$, $\rho_{\max}$, and $\rho_2$ defined
            in Example 3 are uniformly equivalent.
            \begin{align}
                  \rho_1((x_1, y_1), (x_2, y_2))      & \equiv \lvert x_1-x_2\rvert + \lvert y_1-y_2\rvert        \\
                  \rho_2((x_1, y_1), (x_2, y_2))      & \equiv \sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}                   \\
                  \rho_{\max}((x_1, y_1), (x_2, y_2)) & \equiv \max\{\lvert x_1-x_2\rvert, \lvert y_1-y_2\rvert\}
            \end{align}
            \begin{proof}
                  Let $\varepsilon>0$ be given, and $p_1=(x_1,y_1)\in\mathbb{R},p_2=(x_2,y_2)\in \mathbb{R}^2$.
                  Choose $\delta = \frac{\varepsilon}{2}$.
                  Suppose that $\rho_1(p_1,p_2)<\delta$. It follows that
                  \begin{align*}
                        \rho_2(p_1, p_2) & = \sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}                                                                      \\
                                         & = \sqrt{\lvert x_1-x_2\rvert^2 + \lvert y_1-y_2\rvert^2}                                                \\
                                         & \leq \sqrt{\lvert x_1-x_2\rvert^2 + 2\lvert x_1-x_2\rvert\lvert y_1-y_2\rvert + \lvert y_1-y_2\rvert^2} \\
                                         & = \sqrt{(\lvert x_1-x_2\rvert+\lvert y_1-y_2\rvert)^2}                                                  \\
                                         & = \lvert x_1-x_2\rvert+\lvert y_1-y_2\rvert                                                             \\
                                         & = \rho_1(p_1, p_2)                                                                                      \\
                                         & < \delta < \varepsilon
                  \end{align*}
                  and that
                  \begin{align*}
                        \rho_{\max} & = \max\{\lvert x_1-x_2\rvert,\lvert y_1-y_2\rvert\} \\
                                    & \leq 2(\lvert x_1-x_2\rvert+\lvert y_1-y_2\rvert)   \\
                                    & = 2\rho_1(p_1, p_2)                                 \\
                                    & < 2\delta = \varepsilon.
                  \end{align*}
                  On the other hand, suppose that $\rho_2<\delta$ and $\rho_{\max}<\delta$.
                  Then
                  \begin{align*}
                        \rho_1 & = \lvert x_1-x_2\rvert + \lvert y_1-y_2\rvert \\
                               & = \sqrt{(x_1-x_2)^2} + \sqrt{(y_1-y_2)^2}     \\
                               & \leq 2\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}          \\
                               & = 2\rho_2(p_1, p_2)                           \\
                               & < 2\delta = \varepsilon
                  \end{align*}
                  and
                  \begin{align*}
                        \rho_1 & = \lvert x_1-x_2\rvert + \lvert y_1-y_2\rvert           \\
                               & \leq 2\max\{\lvert x_1-x_2\rvert,\lvert y_1-y_2\rvert\} \\
                               & = 2\rho_{\max}(p_1, p_2)                                \\
                               & < 2\delta = \varepsilon
                  \end{align*}
                  Thus, $\rho_1$ is equivalent to $\rho_{\max}$ and $\rho_1$ is equivalent
                  to $\rho_2$. By the transitive property, $\rho_{\max}$ is also equivalent
                  to $\rho_2$.
            \end{proof}
            \setcounter{enumi}{13}
      \item Let $\rho$ be the function defined on $\mathbb{R}\times\mathbb{R}$
            by
            \[
                  \rho(x, y) = \frac{\lvert x-y\rvert}{1+\lvert x-y\rvert}.
            \]
            \begin{enumerate}
                  \item Prove that $\rho$ is a metric.
                        \medbreak
                        By definition, $\lvert x-y\rvert > 0$. It follows that $\rho\geq0$.
                        It also is worth noting that the absolute value function is only
                        $0$ when $x=y$, and therefore $x\neq y\implies \rho>0$ and
                        $x=y\implies\rho=0$. The absolute value function is also
                        symmetrical, so $\rho(x,y)=\rho(y,x)$. Since
                        \begin{align*}
                              \rho(x,z) & = \frac{\lvert x-z\rvert}{1+\lvert x-z\rvert}                                                                               \\
                                        & = \frac{\lvert x-y+y-z\rvert}{1+\lvert x-y+y-z\rvert}                                                                       \\
                                        & = \frac{\lvert x-y\rvert+\lvert y-z\rvert}{1+\lvert x-y\rvert+\lvert y-z\rvert}                                             \\
                                        & = \frac{\lvert x-y\rvert}{1+\lvert x-y\rvert+\lvert y-z\rvert}+\frac{\lvert y-z\rvert}{1+\lvert x-y\rvert+\lvert y-z\rvert} \\
                                        & \leq \frac{\lvert x-y\rvert}{1+\lvert x-y\rvert}+\frac{\lvert y-z\rvert}{1+\lvert y-z\rvert}                                \\
                                        & = \rho(x,y) + \rho(y,z),
                        \end{align*}
                        we see that the triangle inequality holds and therefore $\rho$
                        is a metric on $\mathbb{R}\times\mathbb{R}$.
                  \item Prove that $\rho$ is equivalent to the Euclidean
                        metric $\rho_2$.
                        \begin{proof}
                              Let $\varepsilon>0$ be given and $x, y\in\mathbb{R}$.
                              Choose $\delta=\frac{\varepsilon}{1+\lvert x-y\rvert}$ and suppose that $\rho_2(x,y)<\delta$.
                              It follows that
                              \begin{align*}
                                    \rho(x, y) & = \frac{\lvert x-y\rvert}{1+\lvert x-y\rvert} \\
                                               & \leq \frac{\lvert x-y\rvert}{1}               \\
                                               & = \lvert x-y\rvert                            \\
                                               & = \rho_2(x, y)                                \\
                                               & < \delta \leq \varepsilon
                              \end{align*}
                              On the other hand, suppose that $\rho(x,y)<\delta$. Then
                              \begin{align*}
                                    \rho_2(x, y) & = \lvert x-y\rvert                                                \\
                                                 & = (1+\lvert x-y\rvert)\frac{\lvert x-y\rvert}{1+\lvert x-y\rvert} \\
                                                 & = (1+\lvert x-y\rvert)\rho(x,y)                                   \\
                                                 & < (1+\lvert x-y\rvert)\delta                                      \\
                                                 & = (1+\lvert x-y\rvert)\frac{\varepsilon}{1+\lvert x-y\rvert}      \\
                                                 & = \varepsilon
                              \end{align*}
                        \end{proof}
                  \item Prove that $\rho$ is not uniformly equivalent to
                        $\rho_2$.
                        \begin{proof}
                              Let $\varepsilon>0$ be given. Suppose there exists
                              some $\delta(\varepsilon)$ such that
                              $\rho(x,y)<\delta\implies\rho_2(x,y)<\varepsilon$
                              and $\rho_2(x,y)<\delta\implies\rho(x,y)<\varepsilon$.
                              Suppose $x\neq y$ and $\varepsilon < \lvert x-y\rvert$.
                              Then it follows that, if $\rho(x,y)<\delta$,
                              \begin{align*}
                                    \rho_2(x,y) & = \lvert x-y\rvert                                                \\
                                                & = (1+\lvert x-y\rvert)\frac{\lvert x-y\rvert}{1+\lvert x-y\rvert} \\
                                                & = (1+\lvert x-y\rvert)\rho(x,y)                                   \\
                                                & = \rho(x,y) + \lvert x-y\rvert\rho(x,y)                           \\
                                                & > \rho(x,y) + \varepsilon\rho(x,y)
                              \end{align*}
                              Therefore, $\rho$ and $\rho_2$ are not uniformly equivalent.
                              % TODO: This is not well fleshed out at all and probably isn't correct
                        \end{proof}
            \end{enumerate}
\end{enumerate}

\subsection{The Contraction Mapping Principle}
\begin{enumerate}
      \setcounter{enumi}{3}
      \item Which of the subsets of $\mathbb{R}^2$ are complete metric spaces
            with the Euclidean metric?
            \begin{enumerate}
                  \item $\{(x, y)\in\mathbb{R}^2|x^2+y^2<1 \}$.
                  \item $\{(x, y)\in\mathbb{R}^2|x\geq 1\text{ and }y\leq -2\}$.
                  \item $\{(x, y)\in\mathbb{R}^2|y\in\mathbb{N} \}$.
                  \item $\{(x, y)\in\mathbb{R}^2|f(x,y)=0 \}$, where $f$ is
                        continuous on $\mathbb{R}^2$.
            \end{enumerate}
      \item Which of the following subsets of $C[a,b]$ are complete metric
            spaces with the metric $\rho_\infty$?
            \begin{enumerate}
                  \item $\{f\in C[a,b]|f(x)>0\text{ for }x\in [a,b]\}$.
                  \item $\{f\in C[a,b]|f(a)=0\}$.
                  \item $\{f\in C[a,b]|f(x)=0\text{ for }a<c\leq x\leq d<b\}$.
                  \item $\{f\in C[a,b]|\lvert f(x)\rvert\leq 2+f(x)^2$
                        for $x\in[a,b]\}$.
            \end{enumerate}
            \setcounter{enumi}{9}
      \item \begin{enumerate}
                  \item Prove that if $\rho$ and $\sigma$ are uniformly
                        equivalent metrics on $\mathcal{M}$, then
                        $(\mathcal{M}, \rho)$ is complete only if
                        $(\mathcal{M}, \sigma)$ is complete.
                  \item Suppose that $\rho$ and $\sigma$ are equivalent metrics
                        on $\mathcal{M}$. Show by example that it is possible
                        that $(\mathcal{M}, \rho)$ is complete but
                        $(\mathcal{M}, \sigma)$ is not complete.
            \end{enumerate}
            \setcounter{enumi}{11}
      \item Let $\mathcal{M}$ be the set of continuous functions on $\mathbb{R}$
            which vanish outside a finite interval (the interval may depend on
            the function).
            \begin{enumerate}
                  \item Show that $\mathcal{M}$ is a metric space in the sup
                        norm.
                  \item Show that $\mathcal{M}$ is not complete.
                  \item Show that $C_o(\mathbb{R})$, the continuous functions
                        which go to zero at $\infty$, is complete in the sup
                        norm.
                  \item Prove that $\mathcal{M}$ is dense in $C_o(\mathbb{R})$.
            \end{enumerate}
\end{enumerate}

\subsection{Normed Linear Spaces}
\begin{enumerate}
      \setcounter{enumi}{7}
      \item Two norms, $\lVert\cdot\rVert_1$ and $\lVert\cdot\rVert_2$, are
            called \textbf{equivalent} if there are positive constants, $c$ and
            $d$, so that
            \[
                  c\lVert v\rVert_1\leq\lVert v\rVert_2\leq d\lVert v\rVert_1
            \]
            for all $v\in V$.
            \begin{enumerate}
                  \item Prove that if $\lVert\cdot\rVert_1$ and
                        $\lVert\cdot\rVert_2$  are equivalent, then $V$ is
                        complete in $\lVert\cdot\rVert_1$ if and only if $V$ is
                        complete in $\lVert\cdot\rVert_2$.
                  \item Prove that the $\lVert\cdot\rVert_1$ norm and the
                        Euclidean norm $\lVert\cdot\rVert_2$ are equivalent on
                        $\mathbb{R}^n$ by showing that
                        \[
                              \lVert x\rVert_2^2
                              \leq \lVert x\rVert_1^2
                              \leq n\lVert x\rVert_2^2.
                        \]
                  \item Prove that the sup norm and the $L_1$ norm are not
                        equivalent on $C[a,b]$.
            \end{enumerate}
            \setcounter{enumi}{9}
      \item Let $\{x_i\}_{i=1}^N$ and $\{y_i\}_{i=1}^N$ be real numbers and not
            all zero. Define a quadratic, $p(\lambda)$, by
            \[
                  p(\lambda)=\sum_{i=1}^N(x_i+\lambda y_i)^2.
            \]
            Explain why $p(\lambda)$ has either two complex roots or a double
            real root. Use this fact to prove the \textbf{Cauchy-Schwartz
                  Inequality}
            \begin{equation}
                  \left\lvert\sum_{i=1}^Nx_iy_i\right\rvert \leq
                  \left(\sum_{i=1}^Nx_i^2\right)^\frac{1}{2}
                  \left(\sum_{i=1}^Ny_i^2\right)^\frac{1}{2}.
            \end{equation}
            Under what circumstances does one get equality?
      \item Let $c_o$ denote the set of sequences, ${a_j}$, of real numbers such
            that $a_j\to 0$ as $j\to \infty$. Define
            \[
                  \lVert\{a_j\}_\infty\rVert\equiv\sup_j\lvert a_j\rvert.
            \]
            \begin{enumerate}
                  \item Explain why $c_o$ is a normed linear space with the norm
                        $\lVert\cdot\rVert_\infty$.
                  \item Prove that $c_o$ is complete.
                  \item Show that the set of sequences which are zero after
                        finitely many terms is dense in $c_o$.
                  \item Show that $c_o$ is not dense in $\ell_\infty$.
            \end{enumerate}
            \setcounter{enumi}{13}
      \item For $f\in C^{(2)}[a,b]$, define $\lVert f\rVert
                  \equiv\lVert f\rVert_\infty + \lVert f'\rVert_\infty
                  + \lVert f''\rVert_\infty$.
            \begin{enumerate}
                  \item Explain why $C^{(2)}[a,b]$ is a vector space.
                  \item Prove that $\lVert\cdot\rVert$ is a norm in
                        $C^{(2)}[a,b]$.
                  \item Prove that $C^{(2)}[a,b]$ is complete in the norm
                        $\lVert\cdot\rVert$.
                  \item Prove that $\frac{d^2}{dx^2}$ is a linear
                        transformation from $C^{(2)}[a,b]$ to $C[a,b]$.
                  \item Prove that $\{f\in C^{(2)}[a,b]|f''(x)\equiv 0\}$
                        is a vector space. It is called the \textbf{kernel} of
                        $\frac{d^2}{dx^2}$.
                  \item Identify the functions in the kernel.
                  \item Is the linear transformation $\frac{d^2}{dx^2}$
                        one-to-one?
            \end{enumerate}
\end{enumerate}

\subsection*{Projects}
\begin{enumerate}
      \item The purpose of this project is to prove that if $f$ is a continuous
            function on $[0,1]$, then $\lVert f\rVert_p\to\lVert f\rVert_\infty$
            as $p\to \infty$.
            \begin{enumerate}
                  \item Show that for each $p$,
                        $\lVert f\rVert_p\leq\lVert f\rVert_\infty$.
                  \item Explain why $\lvert f(x)\rvert$ is continuous on
                        $[0,1]$. Let $x_o$ be the point where
                        $\lvert f(x)\rvert$ achieves its maximum. Explain why
                        $\lvert f(x_o)\rvert=\lVert f\rVert_\infty$.
                  \item Assume that $x_o$ is not one of the endpoints and let
                        $\varepsilon>0$ be given. Explain why you can choose
                        a $\mu>0$ so that
                        $\lvert f(x)\rvert=\lVert f\rVert - \varepsilon$ for all
                        $x\in[x_o-\mu,x_o+\mu]$.
                  \item Prove that $\int_0^1\lvert f(x)\rvert^pdx \geq
                              2\mu(\lVert f\rVert_\infty-\varepsilon)^p$ for
                        all $p$.
                  \item Show that for $p$ large enough, $\lVert f\rVert_\infty
                              \geq \lVert f\rVert_p \geq \lVert f\rVert_\infty
                              -2\varepsilon$ and conclude that $lim_{p\to\infty}
                              \lVert f\rVert_p = \lVert f\rVert_\infty$.
            \end{enumerate}
            \setcounter{enumi}{3}
      \item The purpose of this project is to solve the differential equation
            satisfied by the extremal function for the Brachistochrone problem.
            The extremal $y(x)$ satisfies $y(x)(1+(y'(x))^2)=C_1$ for some
            constant $C_1$.
            \begin{enumerate}
                  \item Explain why $C_1<0$ and why $y'(x)$ must blow up as
                        $x\searrow  0$. What does it mean geometrically that
                        $y'(x)$ blows up?
                  \item Prove that $\frac{dx}{dy}=\sqrt{\frac{y}{C_1-y}}$.
                  \item We introduce a new variable $\theta$ as a parameter and
                        try to find $x$ and $y$ in terms of $\theta$. Let $y$
                        and $\theta$ be related by the equation
                        $y=C_1(\sin\theta)^2$. Use the chain rule to prove that
                        \[
                              \frac{dx}{d\theta}=C_1(1-\cos 2\theta).
                        \]
                  \item Find $x(\theta)$ in terms of $C_1$ and a new
                        $C_2$.
                  \item Show that the constants $C_1$ and $C_2$ can be chosen
                        so that the curve $(x(\theta),y(\theta))$ passes through
                        the points $(0, 0)$ and $(x_1, y_1)$.
                  \item Generate the graph of the curve $(x(\theta),y(\theta))$.
                        Why do you think that the curve which gives the shortest
                        time of descent is so steep near the origin?
            \end{enumerate}
\end{enumerate}

\end{document}