\documentclass{article}
% Control margins of the page
% \usepackage[margin=0.5in,top=1in]{geometry}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{titling}
\usepackage{scrextend}


\title{Real Analysis II - Homework I}
\author{Lucas LaValva}
\date{\today}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\theauthor}
\rhead{\thetitle}

\begin{document}
\maketitle

\setcounter{section}{4}

\section{Sequences of Functions}
\subsection{Pointwise and Uniform Convergence}
\begin{enumerate}
      \setcounter{enumi}{1}
      \item Suppose that $g$ is a continuous function on $[a,b]$.
            \begin{enumerate}
                  \item Prove that if $f_n\to f$ pointwise,
                        then $gf_n\to gf$ pointwise.
                        \begin{proof}
                              Because $f_n\to f$ pointwise, we know that,
                              given $\varepsilon > 0$ and $x\in [a,b]$,
                              there exists $N_0$ such that for all
                              $n>N_0$, $\lvert f_n(x)-f(x)\rvert<\varepsilon$.
                              Let $g$ be a continuous function on $[a,b]$, and
                              choose $N_1>N_0\times\max(\lvert g(x)\rvert)$. Then
                              \begin{align*}
                                    \lvert f_n(x)-f(x)\rvert                                         & < \varepsilon                        \\
                                    \lvert g(x)\rvert\lvert f_n(x)-f(x)\rvert                        & < \lvert g(x)\rvert\varepsilon       \\
                                    \lvert g(x)f_n(x)-g(x)f(x)\rvert                                 & < \max(\lvert g(x)\rvert)\varepsilon \\
                                    \frac{\lvert g(x)f_n(x)-g(x)f(x)\rvert}{\max(\lvert g(x)\rvert)} & < \varepsilon
                              \end{align*}
                              We know that $g$ is continuous on $[a,b]$, which implies that
                              $\max(g(x))$ is a real number less than infinity.
                              This shows that there exists $N_0\leq N_1\leq \infty$ such that for
                              all $n>N_1$, $\lvert g(x)f_n(x)-g(x)f(x)\rvert<\varepsilon$.
                        \end{proof}
                  \item Prove that if $f_n\to f$ uniformly,
                        then $gf_n\to gf$ uniformly.
                        \begin{proof}
                              Given $\varepsilon>0$, there exists an $N$ so that
                              $n>N$ implies $\lvert f_n(x)-f(x)\rvert < \varepsilon$
                              for all $x\in [a,b]$. Then
                              \begin{align*}
                                    \lvert f_n(x)-f(x)\rvert                                         & < \varepsilon                        \\
                                    \lvert g(x)\rvert\lvert f_n(x)-f(x)\rvert                        & < \lvert g(x)\rvert\varepsilon       \\
                                    \lvert g(x)f_n(x)-g(x)f(x)\rvert                                 & < \max(\lvert g(x)\rvert)\varepsilon \\
                                    \frac{\lvert g(x)f_n(x)-g(x)f(x)\rvert}{\max(\lvert g(x)\rvert)} & < \varepsilon
                              \end{align*}
                              We know that $g$ is continuous on $[a,b]$, which implies that
                              $\max(g(x))$ is a real number less than infinity.
                              This shows that there exists $N_0\leq N_1\leq \infty$ such that for
                              all $n>N_1$, $\lvert g(x)f_n(x)-g(x)f(x)\rvert<\varepsilon$.

                        \end{proof}
            \end{enumerate}
            \setcounter{enumi}{4}
      \item Prove that $f_n(x)=(x-\frac{1}{n})^2$ converges uniformly
            on any finite interval
            \begin{proof}
                  Let $f(x)=x^2$. Given $\varepsilon>0$, choose $N>\frac{2\max\{\lvert a\rvert, \lvert b\rvert\}+1}{\varepsilon}$. Then, for
                  all $x\in [a,b]$, $n>N$ implies
                  \begin{align*}
                        \lvert f_n(x)-f(x)\rvert & = \left\lvert \left(x-\frac{1}{n}\right)^2-x^2\right\rvert    \\
                                                 & = \left\lvert x^2-2x\frac{1}{n}+\frac{1}{n}^2-x^2\right\rvert \\
                                                 & = \left\lvert \frac{2x}{n}+\frac{1}{n^2}\right\rvert          \\
                                                 & \leq \frac{2\lvert x\rvert}{n}+\frac{1}{n^2}                  \\
                                                 & \leq \frac{2\lvert x\rvert}{n}+\frac{1}{n}                    \\
                                                 & \leq \frac{2\lvert x\rvert+1}{n}                              \\
                                                 & \leq \frac{2\lvert x\rvert+1}{N}                              \\
                                                 & \leq \frac{2\max\{\lvert a\rvert, \lvert b\rvert\}+1}{N}      \\
                                                 & \leq \varepsilon
                  \end{align*}
                  Thus $f_n$ converges uniformly on $[a,b]$.
            \end{proof}
            \setcounter{enumi}{6}
      \item Let $f_n(x) = \frac{nx}{1+n^2x^2}$. Prove that $f_n\to 0$
            pointwise but not uniformly on $[0,1]$.
            \begin{proof}
                  Let $f(x)=0$. Given $\varepsilon>0$ and $x\in[0,1]$,
                  choose $N>\frac{1}{x\varepsilon}$.
                  Then, for all $n>N$, we have the following if $x=0$:
                  \begin{align*}
                        \lvert f_n(x)-f(x)\rvert & = \left\lvert\frac{n(0)}{1+n^2(0)^2}-0\right\rvert \\
                                                 & = \left\lvert\frac{0}{1}\right\rvert               \\
                                                 & = \lvert0\rvert=0\leq\varepsilon
                  \end{align*}
                  and the following if $x\neq 0\implies x>0$:
                  \begin{align*}
                        \lvert f_n(x)-f(x)\rvert & = \left\lvert\frac{nx}{1+n^2x^2}-0\right\rvert \\
                                                 & < \frac{nx}{n^2x^2} = \frac{1}{nx}             \\
                                                 & < \frac{1}{Nx}                                 \\
                                                 & < \varepsilon
                  \end{align*}
                  Therefore, $f_n(x)$ converges pointwise. However, given any
                  $\varepsilon>0$, there exists some $x$ such that
                  $\frac{1}{Nx}\geq\varepsilon$. Therefore, $f_n(x)$ does not
                  converge uniformly.
            \end{proof}
            \setcounter{enumi}{13}
      \item \begin{enumerate}
                  \item Prove that $(1+\frac{x}{n})^n\to e^x$
                        for all $x$.
                        \begin{proof}
                              Let $\varepsilon>0$ be given, and $N>$. Then for all $n\geq N$,
                              \begin{align*}
                                    \left\lvert(1+\frac{x}{n})^n-e^x\right\rvert & = \left\lvert\sum_{k=0}^n\binom{n}{k}\left(\frac{x}{n}\right)^{n-k} - \sum_{k=0}^n\frac{x^k}{k!}\right\rvert \\
                                                                                 & = \left\lvert\sum_{k=0}^n\frac{n!}{k!(n-k)!}\left(\frac{x}{n}\right)^{n-k} - \frac{x^k}{k!}\right\rvert      \\
                                                                                 & = \left\lvert\sum_{k=0}^n\frac{n!x^{(n-k)}}{k!(n-k)!n^{(n-k)}} - \frac{x^k}{k!}\right\rvert                  \\
                                                                                 & = \left\lvert\sum_{k=0}^n\frac{n!x^nn^k}{k!(n-k)!x^kn^n} - \frac{x^k}{k!}\right\rvert                        \\
                                                                                 & \leq \left\lvert\sum_{k=0}^n\frac{n^nx^nn^k}{k!(n-k)!x^kn^n} - \frac{x^k}{k!}\right\rvert                    \\
                                                                                 & = \left\lvert\sum_{k=0}^n\frac{x^nn^k}{k!(n-k)!x^k} - \frac{x^k}{k!}\right\rvert                             \\
                              \end{align*}
                        \end{proof}
                  \item Prove that $(1+\frac{x}{n})^n\to e^x$
                        uniformly on any finite interval $[a,b]$.
                        \begin{proof}
                              % TODO
                              I couldn't figure out how to finish part (a)
                        \end{proof}
                  \item Prove that the convergence is not
                        uniform on $\mathbb{R}$.
                        \begin{proof}
                              % TODO
                              I couldn't figure out how to finish part (a)
                        \end{proof}
            \end{enumerate}
\end{enumerate}

\subsection{Limit Theorems}
\begin{enumerate}
      \setcounter{enumi}{1}
      \item Compute the following limits:
            \begin{enumerate}
                  \item $\lim_{n\to \infty}\int_0^1(x+\frac{1}{n})^2dx$.
                        \begin{align*}
                                & \lim_{n\to \infty}\int_0^1(x+\frac{1}{n})^2dx \\
                              \intertext{$(x+\frac{1}{n})^2$ converges uniformly on $[0,1]$, so we can bring the limit in.}
                              = & \int_0^1\lim_{n\to \infty}(x+\frac{1}{n})^2dx \\
                              = & \int_0^1x^2dx                                 \\
                              = & \left.\frac{x^3}{3}\right\rvert_0^1           \\
                              = & \frac{1^3}{3}-\frac{0^3}{3}                   \\
                              = & \frac{1}{3}
                        \end{align*}
                  \item $\lim_{n\to \infty}\int_1^2e^{-nx}dx$.
                        \begin{align*}
                                & \lim_{n\to \infty}\int_1^2e^{-nx}dx \\
                              \intertext{$e^{-nx}$ converges uniformly on $[1,2]$, so we can bring the limit in.}
                              = & \int_1^2\lim_{n\to \infty}e^{-nx}dx \\
                              = & \int_1^20dx                         \\
                              = & 0
                        \end{align*}
                  \item $\lim_{n\to \infty}\int_0^{\frac{\pi}{2}}
                              \sin (x+\frac{1}{n})dx$.
                        \begin{align*}
                                & \lim_{n\to \infty}\int_0^{\frac{\pi}{2}}\sin (x+\frac{1}{n})dx \\
                              \intertext{$\sin (x+\frac{1}{n})$ converges uniformly on $[0,{\frac{\pi}{2}}]$, so we can bring the limit in.}
                              = & \int_0^{\frac{\pi}{2}}\lim_{n\to \infty}\sin (x+\frac{1}{n})dx \\
                              = & \int_0^{\frac{\pi}{2}}\sin(x)dx                                \\
                              = & \left.-\cos(x)\right\rvert_0^{\frac{\pi}{2}}                   \\
                              = & -\cos\left({\frac{\pi}{2}}\right)+\cos(0)                      \\
                              = & 0 + 1 = 1
                        \end{align*}
                  \item $\lim_{n\to \infty}\int_0^1(1+\frac{x}{n})^ndx$.
                        \begin{align*}
                                & \lim_{n\to \infty}\int_0^1(1+\frac{x}{n})^ndx \\
                              \intertext{$(1+\frac{x}{n})^n$ converges uniformly on $[0,1]$, so we can bring the limit in.}
                              = & \int_0^1\lim_{n\to \infty}(1+\frac{x}{n})^ndx \\
                              = & \int_0^1e^xdx                                 \\
                              = & \left.e^x\right\rvert_0^1                     \\
                              = & e^1-e^0                                       \\
                              = & e-1 \approx 1.732
                        \end{align*}
            \end{enumerate}
            \setcounter{enumi}{3}
      \item Let $\{f_n\}$ be a sequence of continuous functions on a finite
            interval $[a,b]$ that converges uniformly to $f$. Show that for
            all continuous functions $g$ on $[a,b]$,
            \[
                  \lim_{x \to \infty}\int_a^bf_n(t)g(t)dt = \int_a^bf(t)g(t)dt.
            \]
            \begin{proof}
                  Let $\varepsilon>0$ be given. Since $f_n\to f$ uniformly, we
                  know that there exists some $N$ so that for all $n>N$,
                  \[
                        \lvert f_n-f\rvert < \frac{\varepsilon}{\int_a^b\lvert g(x)\rvert dx} \text{for all }x\in [a,b].
                  \]
                  We know that, because $g$ is a continuous function,
                  $\int_a^b\lvert g(x)\rvert dx$ exists for all $a,b\in \mathbb{R}$. Thus,
                  \begin{align*}
                        \left\lvert\int_a^b(f_n(x)g(x))dx-\int_a^b(f(x)g(x))dx\right\rvert & = \left\lvert\int_a^b(f_n(x)g(x)-f(x)g(x))dx\right\rvert                       \\
                                                                                           & \leq \int_a^b\lvert f_n(x)g(x)-f(x)g(x)\rvert dx                               \\
                                                                                           & = \int_a^b\lvert g(x)\rvert\lvert f_n(x)-f(x)\rvert dx                         \\
                                                                                           & < \int_a^b\lvert g(x)\rvert\frac{\varepsilon}{\int_a^b\lvert g(x)\rvert dx} dx \\
                                                                                           & = \frac{\varepsilon}{\int_a^b\lvert g(x)\rvert dx}\int_a^b\lvert g(x)\rvert dx \\
                                                                                           & =\varepsilon
                  \end{align*}
                  Therefore, $\int_a^bf_n(t)g(t)dt\to \int_a^bf(t)g(t)dt$.
            \end{proof}
            \setcounter{enumi}{6}
      \item Suppose that $\{f_n\}$ is a sequence of continuous functions on an
            open interval $(a,b)$ that converges uniformly to $f$ on $(a,b)$.
            Suppose that each $f_n$ is uniformly continuous on $(a,b)$. Prove
            that $f$ is uniformly continuous on $(a,b)$.
            \begin{proof}
                  Let $\varepsilon>0$ be given and $x_o\in(a,b)$. Since $f_n\to f$ uniformly, we
                  know there exists some $N$ so that for all $n>N$,
                  \begin{equation}
                        \lvert f_n-f\rvert < \frac{\varepsilon}{3}
                        \label{eq:5.2_7_uniformConvergence}
                  \end{equation}
                  Since $f_N$ is continuous, we know that there exists some
                  $\delta$ so that
                  \begin{equation}
                        \lvert f_N(x)-f_N(x_o)\rvert < \frac{\varepsilon}{3}
                        \text{ if } \lvert x-x_o\rvert < \delta.
                        \label{eq:5.2_7_continuous}
                  \end{equation}
                  Using \eqref{eq:5.2_7_uniformConvergence} and
                  \eqref{eq:5.2_7_continuous}, we can show that
                  \begin{align*}
                        \lvert f(x)-f(x_o)\rvert & = \lvert f_N(x)-f(x)+f_N(x)-f_N(x_o)+f_N(x_o)-f(x_o)\rvert                              \\
                                                 & \leq \lvert f_N(x)-f(x)\rvert+\lvert f_N(x)-f_N(x_o)\lvert+\rvert f_N(x_o)-f(x_o)\rvert \\
                                                 & < \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3}                 \\
                                                 & = \varepsilon.
                  \end{align*}
                  Therefore, $f$ must be continuous since $\lvert f(x)-f(x_o)\rvert<\varepsilon$.
            \end{proof}
            \setcounter{enumi}{14}
      \item Suppose that ${f_n}$ is a sequence of continuous functions on a
            finite interval $[a,b]$. Suppose that $f_n\to 0$ pointwise
            on $[a,b]$ and that for each $x\epsilon [a,b]$ the sequence of
            numbers $\{f_n(x)\}$ is nonincreasing. Prove that $f_n\to 0$
            uniformly.
            \begin{proof}
                  Let $\varepsilon>0$ be known. Since $f_n\to 0$ pointwise,
                  we know there exists some $N$ for each $x\in[a,b]$ so that for
                  all $n>N$
                  \begin{equation}
                        \lvert f_n(x)-0\rvert<\varepsilon.
                        \label{eq:5.2_15_pointwiseConvergence}
                  \end{equation}
                  We also know that, since $f_n$ is nonincreasing,
                  \begin{equation}
                        \lVert f_n\rVert_\infty \geq \lVert f_{n+k}\rVert_\infty \forall k\in\mathbb{N}
                        \label{eq:5.2_15_nonincreasing}
                  \end{equation}
                  Since this $\{f_n\}$ approaches 0 pointwise, we know that $\lVert f_n\rVert$ appraches 0 as well.
                  Choose $N_C$ so that $\lVert f_{N_C}\rVert_\infty<\varepsilon$. Then, using \eqref{eq:5.2_15_pointwiseConvergence} and
                  \eqref{eq:5.2_15_nonincreasing}, we can show that, for all $n_C>N_C$
                  \begin{align*}
                        \lvert f_{n_C} - 0\rvert & = \lvert f_{n_C}\rvert           \\
                                                 & \leq \lVert f_{N_C}\rVert_\infty \\
                                                 & < \varepsilon
                  \end{align*}
            \end{proof}
\end{enumerate}

\subsection{The Supremum Norm}
\begin{enumerate}
      \setcounter{enumi}{1}
      \item Let $f$ and $g$ be continuous functions on $[a,b]$.
            \begin{enumerate}
                  \item Use the triangle inequality to prove that
                        \[
                              \lvert
                              \lVert f\rVert_\infty - \lVert g\rVert_\infty
                              \rvert \leq \lVert f-g\rVert_\infty
                        \]
                        \begin{proof}
                              We know that
                              \begin{align*}
                                    \lVert f\rVert_\infty                         & = \lVert f-g+g\rVert_\infty                          \\
                                    \lVert f\rVert_\infty                         & \leq \lVert f-g\rVert_\infty + \lVert g\rVert_\infty \\
                                    \lVert f\rVert_\infty - \lVert g\rVert_\infty & \leq \lVert f-g\rVert_\infty                         \\
                                    \intertext{and}
                                    \lVert g\rVert_\infty                         & = \lVert g-f+f\rVert_\infty                          \\
                                    \lVert g\rVert_\infty                         & \leq \lVert g-f\rVert_\infty + \lVert f\rVert_\infty \\
                                    \lVert g\rVert_\infty - \lVert f\rVert_\infty & \leq \lVert g-f\rVert_\infty.                        \\
                              \end{align*}
                              Therefore, because we know that $\lVert f\rVert_\infty - \lVert g\rVert_\infty$
                              and $\lVert g\rVert_\infty - \lVert f\rVert_\infty$ are both less than $\lVert f-g\rVert_\infty$,
                              \[
                                    \lvert
                                    \lVert f\rVert_\infty - \lVert g\rVert_\infty
                                    \rvert \leq \lVert f-g\rVert_\infty.
                              \]
                        \end{proof}
                  \item Suppose that $f_n\to f$ in the sup norm. Prove
                        that $\lVert f_n\rVert_\infty\to\lVert f\rVert_\infty$.
                        \begin{proof}
                              Let $\varepsilon>0$ be given.
                              We know that $f_n\to f$ in the sup norm, so there
                              must exist some $N$ such that for all $n>N$,
                              \begin{equation}
                                    \lVert f_n-f\rVert_\infty<\varepsilon.
                                    \label{eq:5.3_2b_convergence}
                              \end{equation}
                              Using \eqref{eq:5.3_2b_convergence}, and the proposition
                              from part (a), it is clear that
                              \begin{align*}
                                    \lvert\lVert f_n\rVert_\infty-\lVert f\rVert_\infty\rvert & \leq \lVert f_n-f\rVert_\infty \\
                                                                                              & < \varepsilon.
                              \end{align*}
                              Therefore, $\lVert f_n\rVert_\infty\to\lVert f\rVert_\infty$.

                        \end{proof}
            \end{enumerate}
      \item Let $C_b(\mathbb{R})$ denote the set of bounded continuous functions
            on $\mathbb{R}$. Prove that $C_b(\mathbb{R})$ is complete in the sup
            norm.
            \begin{proof}
                  Let $f_o$ be any arbitrary function in $C_b(\mathbb{R})$. Then
                  because $f_o$ it is bounded, there must exist some $y$ such that
                  for all $x$, $f_o(x)<y$. Therefore, $\lVert f_o\rVert_\infty$
                  exists for all functions $f_o$ in $C_b(\mathbb{R})$.
            \end{proof}
            \setcounter{enumi}{4}
      \item Let the functions $f_n$ be defined on $[0,1]$ by
            \begin{equation}
                  f_n(x)=\begin{cases}
                        1
                         & 0\leq x \leq \frac{1}{2}                     \\

                        1-n(x-\frac{1}{2})
                         & \frac{1}{2} < x \leq \frac{1}{2}+\frac{1}{n} \\

                        0 % 
                         & \frac{1}{2}+\frac{1}{n} < x \leq 1
                  \end{cases}
            \end{equation}
            and define
            \begin{equation}
                  f(x)=\begin{cases}
                        1 & 0 \leq x \leq \frac{1}{2} \\
                        0 & \frac{1}{2} < x \leq 1.
                  \end{cases}
            \end{equation}
            \begin{enumerate}
                  \item Prove that $f_n\to f$ pointwise on $[0,1]$.
                        \begin{proof}
                              Let $\varepsilon>0$ be given and $N>\frac{1}{\varepsilon}$. Then, for
                              all $n>N$, the following two cases can be
                              shown: \\
                              Case 1 — $0\leq x \leq \frac{1}{2}\implies$
                              \begin{align*}
                                    f_n(x)             & = 1    \\
                                    f(x)               & = 1    \\
                                    \therefore  f_n(x) & = f(x)
                              \end{align*}
                              Case 2 — $\frac{1}{2}< x \leq 1.$ \\
                              Let us assume that $\frac{1}{2}< x \leq\frac{1}{2} + \frac{1}{n}$.
                              Then
                              \begin{align*}
                                    \lvert\frac{1}{n}-0\rvert & = \lvert\frac{1}{n}\rvert \\
                                                              & = \frac{1}{n}             \\
                                                              & < \frac{1}{N}             \\
                                                              & < \varepsilon.
                              \end{align*}
                              Therefore,
                              \[
                                    \frac{1}{2}< x \leq\frac{1}{2}
                              \]
                              This clearly cannot be true. Contradiction!
                              Thus,
                              \begin{align*}
                                    \frac{1}{2} + \frac{1}{n}< & x \leq 1 \\
                                    f_n(x)                     & = 0      \\
                                    f(x)                       & = 0      \\
                                    \therefore f_n(x)          & = f(x)
                              \end{align*}
                              Therefore, $f_n(x)\to f(x)$ pointwise.
                        \end{proof}
                  \item Prove that $\lVert f-f_n\rVert_\infty=1$ for each $n$ so
                        that $f_n$ does not converge to $f$ in the $sup$ norm.
                        \begin{proof}
                              \begin{align*}
                                    \lvert f-f_n\rvert                   & = \begin{cases}
                                          0                  & 0 \leq x \leq \frac{1}{2}                  \\
                                          1-n(x-\frac{1}{2}) & \frac{1}{2}<x \leq \frac{1}{2}+\frac{1}{n} \\
                                          0                  & \frac{1}{2}+\frac{1}{n} < x \leq 1
                                    \end{cases}             \\
                                    \therefore \lVert f-f_n\rVert_\infty & = \lVert 1-n(x-\frac{1}{2})\rVert_\infty
                              \end{align*}
                              For each $n$, choose $x_o=\frac{1}{2n}+\frac{1}{2}$. Then,
                              because $x_o\leq \frac{1}{2}+\frac{1}{n}$, we can show that
                              \begin{align*}
                                    \lVert f-f_n\rVert_\infty & = \lVert 1-n(x_o-\frac{1}{2})\rVert_\infty                      \\
                                                              & = \lVert 1-n(\frac{1}{2n}+\frac{1}{2}-\frac{1}{2})\rVert_\infty \\
                                                              & = \lVert 1-n(\frac{1}{2n})\rVert_\infty                         \\
                                                              & = \lVert 1-\frac{1}{2}\rVert_\infty                             \\
                                                              & = \frac{1}{2}
                              \end{align*}
                        \end{proof}
                  \item Explain how you could have predicted the result of part
                        (b) simply by using theorem 5.2.1.
                        \begin{description}
                              \item[Theorem 5.2.1] Let $\{f_n\}$ be a continuous
                                    set of functions on a set $E\subseteq\mathbb{R}$.
                                    If $f_n\to f$ uniformly on $E$ as $n\to\infty$ then
                                    $f$ is continuous on $E$.
                        \end{description}
                        \begin{proof}[Modus Ponens!]
                              The antecendent of theorem 5.2.1 tells us that if $f$ is NOT
                              continuous on $E$, then $f_n\nrightarrow f$ uniformly.
                              $f$ is not continuous on $E$, so this is clearly the case.
                        \end{proof}
                  \item Prove that $\lVert f-f_n\rVert_1\to 0$ as $n\to \infty$.
                        \begin{proof}
                              Let $\varepsilon>0$ be given, and choose $N>\frac{2}{\varepsilon}$.
                              Then, for all $n>N$,
                              \begin{align*}
                                    \lVert f-f_n\rVert_1 & = \int_0^\frac{1}{2}0dx+\int_\frac{1}{2}^{\frac{1}{2}+\frac{1}{n}}1-n(x-\frac{1}{2})dx+\int_{\frac{1}{2}+\frac{1}{n}}^10dx                                          \\
                                                         & = \int_\frac{1}{2}^{\frac{1}{2}+\frac{1}{n}}1-n(x-\frac{1}{2})dx                                                                                                    \\
                                                         & = \left.\left(1+\frac{n}{2}\right)x-\frac{nx^2}{2}\right\rvert_\frac{1}{2}^{\frac{1}{2}+\frac{1}{n}}                                                                \\
                                                         & = \left(1+\frac{n}{2}\right){(\frac{1}{2}+\frac{1}{n})}-\frac{n{(\frac{1}{2}+\frac{1}{n})}^2}{2} - \frac{1}{2}\left(1+\frac{n}{2}\right)+\frac{n(\frac{1}{2})^2}{2} \\
                                                         & = \frac{1}{2} + \frac{1}{n} + \frac{n}{4} + \frac{1}{2} - \frac{n}{8} - \frac{1}{2} - \frac{1}{2n} - \frac{1}{2} - \frac{n}{4} + \frac{n}{8}                        \\
                                                         & = \frac{1}{n} + \frac{1}{2n}                                                                                                                                        \\
                                                         & \leq \frac{1}{n}+ \frac{1}{n}                                                                                                                                       \\
                                                         & = \frac{2}{n}                                                                                                                                                       \\
                                                         & \leq \frac{2}{N}                                                                                                                                                    \\
                                                         & \leq \varepsilon
                              \end{align*}
                        \end{proof}
            \end{enumerate}
            \setcounter{enumi}{6}
      \item Let $C^{(1)}[a,b]$ denote the set of continuously differentiable
            functions on a finite interval $[a,b]$. For each $f$ in
            $C^{(1)}[a,b]$, define $\lVert f\rVert \equiv \lVert f\rVert_\infty
                  + \lVert f'\rVert_\infty$.
            \begin{enumerate}
                  \item Show that $\lVert f\rVert$ has the properties (a), (b),
                        and (c) of Proposition 5.3.1.
                        \begin{description}
                              \item[(a)] $\lVert f\rVert_\infty\geq 0$ and
                                    $\lVert f\rVert_\infty=0$ if and only if $f$
                                    is the zero function on $E$.
                                    \begin{proof}
                                          Let $f$ be any arbitrary function on
                                          $C^{(1)}[a,b]$. Because $C^{(1)}[a,b]$
                                          is continuously differentiable we know
                                          that $f$ and $f'$ are both continuous,
                                          and thus able to be measured by the sup
                                          norm. Therefore,
                                          \begin{align*}
                                                (\lVert f\rVert_\infty \geq 0) & \land (\lVert f'\rVert_\infty  \geq 0) & \implies\lVert f\rVert_\infty + \lVert f'\rVert_\infty & \geq 0 \\
                                                (\lVert f\rVert_\infty = 0)    & \land (\lVert f'\rVert_\infty  = 0)    & \implies\lVert f\rVert_\infty + \lVert f'\rVert_\infty & = 0    \\
                                                (\lVert f\rVert_\infty > 0)    & \lor (\lVert f'\rVert_\infty  > 0)     & \implies\lVert f\rVert_\infty + \lVert f'\rVert_\infty & > 0
                                          \end{align*}
                                    \end{proof}
                              \item[(b)] For every $\alpha\in\mathbb{R}$, we have
                                    $\lVert\alpha f\rVert_\infty=\lvert\alpha\rvert\lVert f\rVert_\infty$.
                                    \begin{proof}
                                          \begin{align*}
                                                \lvert\alpha\rvert\lVert f\rVert & = \lvert\alpha\rvert(\lVert f\rVert_\infty+\lVert f'\rVert_\infty)                 \\
                                                                                 & = \lvert\alpha\rvert\lVert f\rVert_\infty+\lvert\alpha\rvert\lVert f'\rVert_\infty \\
                                                                                 & = \lVert\alpha f\rVert_\infty+\lVert\alpha f'\rVert_\infty                         \\
                                                                                 & = \lVert\alpha f\rVert
                                          \end{align*}
                                    \end{proof}
                              \item[(c)] $\lVert f+g\rVert_\infty\leq\lVert f\rVert_\infty+\lVert g\rVert_\infty$
                                    (the triangle inequality).
                                    \begin{proof}
                                          \begin{align*}
                                                \lVert f+g\rVert & = \lVert f+g\rVert_\infty+\lVert (f+g)'\rVert_\infty                                                  \\
                                                                 & = \lVert f+g\rVert_\infty+\lVert f'+g'\rVert_\infty                                                   \\
                                                                 & = \lVert f+g\rVert_\infty+\lVert f'+g'\rVert_\infty                                                   \\
                                                                 & \leq \lVert f\rVert_\infty + \lVert g\rVert_\infty + \lVert f'\rVert_\infty + \lVert g'\rVert_\infty  \\
                                                                 & = (\lVert f\rVert_\infty + \lVert f'\rVert_\infty) + (\lVert g\rVert_\infty + \lVert g'\rVert_\infty) \\
                                                                 & = \lVert f\rVert + \lVert g\rVert
                                          \end{align*}
                                    \end{proof}
                        \end{description}
                  \item Prove that $C^{(1)}[a,b]$ is complete in the norm
                        $\lVert f\rVert$.
                        \begin{proof}
                              Let $\{f_n\}$ be a Cauchy sequence in the norm
                              $\lVert f\rVert$, and $\varepsilon>0$ be given.
                              Because $\{f_n\}$ is Cauchy, there exists some $N$
                              such that, for all $n,m>N$,
                              \begin{equation}
                                    \lVert f_n-f_m\rVert<\varepsilon.
                              \end{equation}
                              Thus, for each $x\in\mathbb{R}$, since
                              \begin{equation}
                                    \lvert f_n(x)-f_m(x)\rvert \leq \lVert f_n(x)-f_m(x)\rVert
                              \end{equation}
                              we know that
                              \begin{equation}
                                    \lvert f_n(x)-f_m(x)\rvert < \varepsilon
                                    \label{eq:5.3_7b_abs}
                              \end{equation}
                              Define $f(x)\equiv \lim_{n\to\infty}f_n(x)$. The
                              absolute value is a continuous function, so
                              \begin{equation*}
                                    \lvert f(x)-f_m(x)\rvert = \lim_{n\to\infty}\lvert f_n(x)-f_m(x)\rvert.
                              \end{equation*}
                              Then, \eqref{eq:5.3_7b_abs} implies that for all
                              $m>N$ and $x\in[a,b]$
                              \[
                                    \lvert f(x)-f_m(x)\rvert \leq \varepsilon
                              \]
                              This shows that $f_m\to f$ uniformly. Therefore,
                              $f$ is continuous and $f_n$ converges to $f$ in
                              the norm $\lVert f\rVert$.
                        \end{proof}
            \end{enumerate}
\end{enumerate}

\setcounter{subsection}{4}
\subsection{The Calculus of Variations}
\begin{enumerate}
      \item Suppose that $H(x)$ is a continuous function on the interval
            $[x_1, x_2]$ and that
            \begin{equation}
                  \int_{x_1}^{x_2}H(x)\eta (x)dx = 0
                  \label{eq:5.5_e1}
            \end{equation}
            for all twice continuously differentiable functions $\eta (x)$ that
            vanish at the endpoints. Prove that $H(x)\equiv 0$ in the interval
            as follows:
            \begin{enumerate}
                  \item Let $[a,b]$ be any finite interval. Show how to
                        construct a twice continuously differentiable function
                        on $\mathbb{R}$ which is strictly positive on the open
                        interval $(a,b)$ and identically zero everywhere else.
                        \begin{align*}
                              f(x)   & =\begin{cases}
                                    0               & \text{if } x \leq a  \\
                                    -(x-a)^3(x-b)^3 & \text{if } a < x < b \\
                                    0               & \text{if } b \leq x
                              \end{cases}  \\
                              f'(x)  & =\begin{cases}
                                    0                                  & \text{if } x \leq a  \\
                                    -3(x-a)^3(x-b)^2 - 3(x-a)^2(x-b)^3 & \text{if } a < x < b \\
                                    0                                  & \text{if } b \leq x
                              \end{cases}  \\
                              f''(x) & =\begin{cases}
                                    0                          & \text{if } x \leq a \\
                                    \begin{matrix}
                                          -6(x-a)(x-b)^3 - 18(x-a)^2(x-b)^2 \\
                                          - 6(x-a)^3(x-b)
                                    \end{matrix} &
                                    \text{if } a < x < b                             \\
                                    0                          & \text{if } b \leq x
                              \end{cases} \\
                        \end{align*}
                  \item If $x_o$ is a point of $[x_1, x_2]$ such that $H(x_o)
                              \neq 0$, show how to choose $\eta (x)$ so that
                        hypothesis \eqref{eq:5.5_e1} is violated.
                        \begin{proof}
                              Any $\eta$ that contains no zeroes in
                              $(a,b)$ will violate \eqref{eq:5.5_e1} if there
                              exists some $x_o$ in which $H(x_o)\neq 0$. This
                              is because $H(x_o)\eta(x_o)\neq 0$, which implies
                              that the integral cannot be 0. % TODO: CAN BE IMPROVED
                        \end{proof}
            \end{enumerate}
            \setcounter{enumi}{2}
      \item Find a curve passing through $(1,2)$ and $(2,4)$ that is an extremal
            for the functional
            \begin{equation}
                  J(x, y') = \int_1^2xy'(x)+(y'(x))^2dx.
            \end{equation}
            \begin{proof}
                  Let $f(x, y, y')=xy'+(y')^2$. Using the Euler-Lagrange equation,
                  \begin{align*}
                        f_y(x, y') - \frac{d}{dx}f_{y'}(x, y')
                         & = 0 - \frac{d}{dx}(x+2y') \\
                         & = -(1+2y'')               \\
                         & = -2y'' - 1.
                  \end{align*}
                  Therefore,
                  \begin{align*}
                        -2y'' - 1 & = 0                        \\
                        y''       & = -\frac{1}{2}             \\
                        y'        & = -\frac{x}{2} + c_1       \\
                        y         & = -\frac{x^2}{4}+xc_1+c_2.
                  \end{align*}
                  To find the values of the constants, we are left with the following
                  system of equations:
                  \begin{align*}
                        2           & = -\frac{1^2}{4}+(1)c_1+c_2                \\
                        4           & = -\frac{2^2}{4}+(2)c_1+c_2                \\
                        \\
                        \frac{9}{4} & = c_1+c_2                                  \\
                        5           & = 2c_1+c_2                                 \\
                        \\
                        c_1         & = \frac{11}{4}                             \\
                        c_2         & = -\frac{1}{2}                             \\
                        \\
                        \therefore
                        y(x)        & = -\frac{x^2}{4}+\frac{11x}{4}-\frac{1}{2}
                  \end{align*}
            \end{proof}
            \setcounter{enumi}{4}
      \item Find a curve passing through $(0,0)$ and $(1,1)$ that is an extremal
            for the functional
            \begin{equation}
                  J(x, y, y') = \int_0^1(y'(x))^2+12xy(x)dx.
            \end{equation}
            \begin{proof}
                  Let $f(x, y, y')=(y')^2+12xy$. Using the Euler-Lagrange equation,
                  \begin{align*}
                        f_y(x, y') - \frac{d}{dx}f_{y'}(x, y')
                         & = 12x - \frac{d}{dx}2y' \\
                         & = 12x - 2y''.
                  \end{align*}
                  Therefore,
                  \begin{align*}
                        12x - 2y'' & = 0                 \\
                        y''        & = 6x                \\
                        y'         & = 3x^2 + c_1        \\
                        y          & = x^3 + xc_1 + c_2.
                  \end{align*}
                  We are left with the following system of equations:
                  \begin{align*}
                        0    & = (0)^3 + (0)c_1 + c_2 \\
                        1    & = (1)^3 + (1)c_1 + c_2 \\
                        \\
                        0    & = c_2                  \\
                        1    & = 1 + c_1 + c_2        \\
                        \\
                        c_1  & = 0                    \\
                        c_2  & = 0                    \\
                        \\
                        \therefore
                        y(x) & = x^3
                  \end{align*}
            \end{proof}
            \setcounter{enumi}{7}
      \item Let $y(x)$ be a twice differentiable function whose graph passes
            through the points $(x_1,y_1)$ and $(x_2,y_2)$ in the plane, where
            $y_1>0$ and $y_2>0$.
            \begin{enumerate}
                  \item Show that the surface area generated when the curve is
                        revolved around the $x$-axis is given by
                        \begin{equation}
                              J(y, y') = 2\pi\int_{x_1}^{x_2}y(x)
                              \sqrt{1+(y'(x))^2}dx.
                        \end{equation}
                        \begin{proof}
                              For any given $x_a$, $x_b$, where $x_1\leq x_a<x_b\leq x_2$,
                              the there exists some frustrum of a cone generated when the segment
                              between the points at $(x_a, y(x_a))$ and $(x_b, y(x_b))$ is revolved
                              around the $x$-axis. To find the surface area of this frustrum,
                              we can use
                              \begin{align*}
                                    A & = \frac{(\textrm{circumference of $x_a$'s circle}) + (\textrm{circumference of $x_b$'s circle})}{2} \\
                                      & \phantom{=} \times (\textrm{length of diagonal})                                                    \\
                                      & = \frac{2\pi y(x_a) + 2\pi y(x_b)}{2}\sqrt{(x_a-x_b)^2+((y(x_a)-y(x_b))^2)}                         \\
                                      & = \pi(y(x_a)+y(x_b))\sqrt{\Delta x^2+\Delta y^2}                                                    \\
                                      & = \pi(y(x_a)+y(x_b))\sqrt{\Delta x^2+(\frac{\Delta x\Delta y}{\Delta x})^2}                         \\
                                      & = \pi(y(x_a)+y(x_b))\sqrt{\Delta x^2+\Delta x^2(\frac{\Delta y}{\Delta x})^2}                       \\
                                      & = \pi(y(x_a)+y(x_b))\sqrt{1+(\frac{\Delta y}{\Delta x})^2}\Delta x.
                              \end{align*}
                              As $x_a\to x_b$, $A\to 2\pi y(x)\sqrt{1+(\frac{dy}{dx})^2}dx$. Then we can denote
                              the surface area of this integral as
                              \[
                                    2\pi\int_{x_1}^{x_2}y(x)\sqrt{1+(y'(x))^2}dx.
                              \]
                        \end{proof}
                  \item Show that if $y(x)$ is an extremal, then $y(x)$
                        satisfies
                        \[
                              \frac{y(x)}{\sqrt{1+(y'(x))^2}}=C_1.
                        \]
                        \begin{proof}
                              If $y(x)$ is an extremal, then there exists some
                              $f(x,y,y')$ such that
                              \[
                                    \int_{x_1}^{x_2} f(x,y(x),y'(x))dx
                              \]
                              has a local maximum or minimum at $y(x)$. Then,
                              by the Euler-Lagrange equation,
                              \[
                                    f_y(x,y,y') - \frac{d}{dx}f_{y'}(x,y,y')=0.
                              \]
                              % TODO: I truly don't understand this one. Is part A even involved?
                        \end{proof}
                  \item Show that the functions $y(x)=C_2\cosh
                              (\frac{x-C_1}{C_2})$ satisfy the differential
                        equation for all choices of $C_1$ and $C_2$.
                  \item Show that $C_1$ and $C_2$ can be chosen so that the
                        graph of $y$ passes through $(x_1, y_1)$ and
                        $(x_2, y_2)$.
            \end{enumerate}
\end{enumerate}

\subsection{Metric Spaces}
\begin{enumerate}
      \item Prove that the functions $\rho_1$ and $\rho_{\max}$ defined in
            example 3 are indeed metrics on $\mathbb{R}^2$.
            \begin{align*}
                  \rho_1((x_1, y_1), (x_2, y_2))      & \equiv \lvert x_1-x_2\rvert + \lvert y_1-y_2\rvert        \\
                  \rho_{\max}((x_1, y_1), (x_2, y_2)) & \equiv \max\{\lvert x_1-x_2\rvert, \lvert y_1-y_2\rvert\}
            \end{align*}
            \begin{proof}
                  To prove that $\rho_1$ is a metric, we must show that it satisfies
                  the following criteria:
                  \begin{enumerate}
                        \item $\forall (x_1,y_1),(x_2,y_2)\in\mathbb{R}^2, \rho_1((x_1,y_1),(x_2,y_2))\geq 0$
                              \medbreak
                              By definition, $\lvert x_1-x_2\rvert\geq 0$ and $\lvert y_1-y_2\rvert\geq 0$.
                              Therefore, $\lvert x_1-x_2\rvert+\lvert y_1-y_2\rvert\geq 0$
                              \medbreak
                              $\rho_1=0$ iff $(x_1,y_1)=(x_2,y_2)$.
                              \medbreak
                              Let us assume that $(x_1,y_1)\neq (x_2,y_2)$. Then at least one of $\lvert x_1-x_2\rvert\neq 0$
                              $\lvert y_1-y_2\rvert\neq 0$ is true. Therefore, $\lvert x_1-x_2\rvert+\lvert y_1-y_2\rvert> 0$.
                              However, if $(x_1,y_1) = (x_2,y_2)$, then $\lvert x_1-x_2\rvert = \lvert y_1-y_2\rvert = 0$ and
                              therefore $\rho_1((x_1,y_1),(x_2,y_2))=0$

                        \item $\rho_1((x_1,y_1),(x_2,y_2)) = \rho_1((x_2,y_2),(x_1,y_1))$
                              \medbreak
                              By the commutative property of addition,
                              $\lvert x_1-x_2\rvert+\lvert y_1-y_2\rvert=\lvert y_1-y_2\rvert+\lvert x_1-x_2\rvert$

                        \item $\forall(x_1,y_1),(x_2,y_2),(x_3,y_3)\in\mathbb{R}^2,$ \\
                              $\rho_1((x_1,y_1),(x_3,y_3))\leq \rho_1((x_1,y_1),(x_2,y_2))+\rho_1((x_2,y_2),(x_3,y_3))$
                              \begin{align*}
                                    \rho_1((x_1, y_1),(x_3,y_3)) & = \lvert x_1-x_3\rvert+\lvert y_1-y_3\rvert                                              \\
                                                                 & = \lvert x_1-x_2+x_2-x_3\rvert+\lvert y_1-y_2+y_2-y_3\rvert                              \\
                                                                 & \leq \lvert x_1-x_2\rvert+\lvert x_2-x_3\rvert+\lvert y_1-y_2\rvert+\lvert y_2-y_3\rvert \\
                                                                 & = \rho_1((x_1,y_1),(x_2,y_2))+\rho_1((x_2,y_2),(x_3,y_3))
                              \end{align*}
                  \end{enumerate}
            \end{proof}
            \begin{proof}
                  To prove that $\rho_{\max}$ is a metric, we must show that it satisfies
                  the following criteria:
                  \begin{enumerate}
                        \item $\forall (x_1,y_1),(x_2,y_2)\in\mathbb{R}^2, \rho_{\max}((x_1,y_1),(x_2,y_2))\geq 0$
                              \medbreak
                              By definition, $\lvert x_1-x_2\rvert\geq 0$ and $\lvert y_1-y_2\rvert\geq 0$.
                              Therefore, $\max(\lvert x_1-x_2\rvert,\lvert y_1-y_2\rvert)\geq 0$
                              \medbreak
                              $\rho_{\max}=0$ iff $(x_1,y_1)=(x_2,y_2)$
                              \medbreak
                              Let us assume that $(x_1,y_1)\neq (x_2,y_2)$. Then at least one of $\lvert x_1-x_2\rvert\neq 0$
                              $\lvert y_1-y_2\rvert\neq 0$ is true. Therefore, $\max(\lvert x_1-x_2\rvert,\lvert y_1-y_2\rvert)> 0$.
                              However, if $(x_1,y_1) = (x_2,y_2)$, then $\lvert x_1-x_2\rvert = \lvert y_1-y_2\rvert = 0$ and
                              therefore $\rho_{\max}((x_1,y_1),(x_2,y_2))=0$
                        \item $\rho_{\max}((x_1,y_1),(x_2,y_2)) = \rho_{\max}((x_2,y_2),(x_1,y_1))$
                              \medbreak
                              By definition, $\max$ is commutative and therefore $\rho_{max}$ is commutative.
                        \item $\forall(x_1,y_1),(x_2,y_2),(x_3,y_3)\in\mathbb{R}^2,\rho_{\max}((x_1,y_1),(x_3,y_3))$\\
                              \phantom{tabbing}$\leq \rho_{\max}((x_1,y_1),(x_2,y_2))+\rho_{\max}((x_2,y_2),(x_3,y_3))$
                              \begin{align*}
                                     & \rho_{\max}((x_1,y_1),(x_3,y_3))                                       \\
                                     & \leq \lvert(x_1,y_1)\rvert + \lvert(x_3,y_3)\rvert                     \\
                                     & \leq \rho_{\max}((x_1,y_1),(x_2,y_2))+\rho_{\max}((x_2,y_2),(x_3,y_3))
                              \end{align*}

                  \end{enumerate}
            \end{proof}
            \setcounter{enumi}{9}
      \item A metric space $(\mathcal{M}, \rho)$ is said to be \textit{discrete}
            if for every $x\epsilon\mathcal{M}$ there is an $\varepsilon>0$ so
            that $\rho(x, y)<\varepsilon$ implies $y=x$.
            \begin{enumerate}
                  \item Define a function $\delta$ on $\mathbb{R}$ by
                        $\delta(x,x)=0$ and $\delta(x, y)=1$ if $x\neq y$. Prove
                        that $(\mathbb{R}, \delta)$ is discrete.
                        \medbreak
                        $\delta(a,b)=\begin{cases}
                                    0 & \textrm{if } a = b    \\
                                    1 & \textrm{if } a \neq b
                              \end{cases}$
                        \begin{proof}
                              Let $\varepsilon>0$. By definition, if $x\neq y$ then
                              $\delta(x,y)=1$. Therefore, if $\varepsilon\leq 1$,
                              $x\neq y\implies \delta(x,y)\geq\varepsilon$.
                              Thus, the transposition is true:
                              $\delta(x,y)<\varepsilon\implies x=y$ for all $\varepsilon<1$.
                        \end{proof}
                  \item Prove that $(\mathbb{R}, \rho_2)$ is not discrete.
                        \begin{equation}
                              (\mathbb{R}^n,\rho_2(x,y)) = \sqrt{\sum_{i=1}^n\lvert x_i-y_i\rvert^2}
                        \end{equation}
                        \begin{proof}
                              On $\mathbb{R}^1$,
                              \begin{align*}
                                    \rho_2(x,y) & = \sqrt{\lvert x-y\rvert^2} \\
                                                & = \lvert x-y\rvert
                              \end{align*}
                              Let $\varepsilon>0$, and $x\in\mathbb{R}$. Then
                              \begin{align*}
                                    \rho_2(x, x+\frac{\varepsilon}{2}) & = \left\lvert x-(x+\frac{\varepsilon}{2})\right\rvert \\
                                                                       & = \left\lvert -\frac{\varepsilon}{2}\right\rvert      \\
                                                                       & = \frac{\varepsilon}{2}                               \\
                                                                       & < \varepsilon
                              \end{align*}
                              Because $\varepsilon>0$, $x+\frac{\varepsilon}{2}\neq x$. Therefore, $\rho_2$
                              is not discrete.
                        \end{proof}
                  \item Which of the metrics in Examples $1-5$ are discrete?
                        \medbreak
                        Most metrics in these examples are not discrete. The only one I found
                        was the one in example 5, where $\delta(q, p)$ is essentially normalized hamming
                        distance:
                        \begin{equation}
                              \rho(x,y) \equiv \sum_{i=1}^N\delta(q_i, p_i)
                        \end{equation}
                  \item In a discrete metric space $(\mathcal{M}, \rho)$, what
                        are the convergent sequences?
                        \medbreak
                        The only convergent sequences in discrete metrics are those
                        that are constant under the metric space.
            \end{enumerate}
            \setcounter{enumi}{11}
      \item Prove that the metrics $\rho_1$, $\rho_{\max}$, and $\rho_2$ defined
            in Example 3 are uniformly equivalent.
            \begin{align}
                  \rho_1((x_1, y_1), (x_2, y_2))      & \equiv \lvert x_1-x_2\rvert + \lvert y_1-y_2\rvert        \\
                  \rho_2((x_1, y_1), (x_2, y_2))      & \equiv \sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}                   \\
                  \rho_{\max}((x_1, y_1), (x_2, y_2)) & \equiv \max\{\lvert x_1-x_2\rvert, \lvert y_1-y_2\rvert\}
            \end{align}
            \begin{proof}
                  Let $\varepsilon>0$ be given, and $p_1=(x_1,y_1)\in\mathbb{R},p_2=(x_2,y_2)\in \mathbb{R}^2$.
                  Choose $\delta = \frac{\varepsilon}{2}$.
                  Suppose that $\rho_1(p_1,p_2)<\delta$. It follows that
                  \begin{align*}
                        \rho_2(p_1, p_2) & = \sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}                                                                      \\
                                         & = \sqrt{\lvert x_1-x_2\rvert^2 + \lvert y_1-y_2\rvert^2}                                                \\
                                         & \leq \sqrt{\lvert x_1-x_2\rvert^2 + 2\lvert x_1-x_2\rvert\lvert y_1-y_2\rvert + \lvert y_1-y_2\rvert^2} \\
                                         & = \sqrt{(\lvert x_1-x_2\rvert+\lvert y_1-y_2\rvert)^2}                                                  \\
                                         & = \lvert x_1-x_2\rvert+\lvert y_1-y_2\rvert                                                             \\
                                         & = \rho_1(p_1, p_2)                                                                                      \\
                                         & < \delta < \varepsilon
                  \end{align*}
                  and that
                  \begin{align*}
                        \rho_{\max} & = \max\{\lvert x_1-x_2\rvert,\lvert y_1-y_2\rvert\} \\
                                    & \leq 2(\lvert x_1-x_2\rvert+\lvert y_1-y_2\rvert)   \\
                                    & = 2\rho_1(p_1, p_2)                                 \\
                                    & < 2\delta = \varepsilon.
                  \end{align*}
                  On the other hand, suppose that $\rho_2<\delta$ and $\rho_{\max}<\delta$.
                  Then
                  \begin{align*}
                        \rho_1 & = \lvert x_1-x_2\rvert + \lvert y_1-y_2\rvert \\
                               & = \sqrt{(x_1-x_2)^2} + \sqrt{(y_1-y_2)^2}     \\
                               & \leq 2\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}          \\
                               & = 2\rho_2(p_1, p_2)                           \\
                               & < 2\delta = \varepsilon
                  \end{align*}
                  and
                  \begin{align*}
                        \rho_1 & = \lvert x_1-x_2\rvert + \lvert y_1-y_2\rvert           \\
                               & \leq 2\max\{\lvert x_1-x_2\rvert,\lvert y_1-y_2\rvert\} \\
                               & = 2\rho_{\max}(p_1, p_2)                                \\
                               & < 2\delta = \varepsilon
                  \end{align*}
                  Thus, $\rho_1$ is equivalent to $\rho_{\max}$ and $\rho_1$ is equivalent
                  to $\rho_2$. By the transitive property, $\rho_{\max}$ is also equivalent
                  to $\rho_2$.
            \end{proof}
            \setcounter{enumi}{13}
      \item Let $\rho$ be the function defined on $\mathbb{R}\times\mathbb{R}$
            by
            \[
                  \rho(x, y) = \frac{\lvert x-y\rvert}{1+\lvert x-y\rvert}.
            \]
            \begin{enumerate}
                  \item Prove that $\rho$ is a metric.
                        \medbreak
                        By definition, $\lvert x-y\rvert > 0$. It follows that $\rho\geq0$.
                        It also is worth noting that the absolute value function is only
                        $0$ when $x=y$, and therefore $x\neq y\implies \rho>0$ and
                        $x=y\implies\rho=0$. The absolute value function is also
                        symmetrical, so $\rho(x,y)=\rho(y,x)$. Since
                        \begin{align*}
                              \rho(x,z) & = \frac{\lvert x-z\rvert}{1+\lvert x-z\rvert}                                                                               \\
                                        & = \frac{\lvert x-y+y-z\rvert}{1+\lvert x-y+y-z\rvert}                                                                       \\
                                        & = \frac{\lvert x-y\rvert+\lvert y-z\rvert}{1+\lvert x-y\rvert+\lvert y-z\rvert}                                             \\
                                        & = \frac{\lvert x-y\rvert}{1+\lvert x-y\rvert+\lvert y-z\rvert}+\frac{\lvert y-z\rvert}{1+\lvert x-y\rvert+\lvert y-z\rvert} \\
                                        & \leq \frac{\lvert x-y\rvert}{1+\lvert x-y\rvert}+\frac{\lvert y-z\rvert}{1+\lvert y-z\rvert}                                \\
                                        & = \rho(x,y) + \rho(y,z),
                        \end{align*}
                        we see that the triangle inequality holds and therefore $\rho$
                        is a metric on $\mathbb{R}\times\mathbb{R}$.
                  \item Prove that $\rho$ is equivalent to the Euclidean
                        metric $\rho_2$.
                        \begin{proof}
                              Let $\varepsilon>0$ be given and $x, y\in\mathbb{R}$.
                              Choose $\delta=\frac{\varepsilon}{1+\lvert x-y\rvert}$ and suppose that $\rho_2(x,y)<\delta$.
                              It follows that
                              \begin{align*}
                                    \rho(x, y) & = \frac{\lvert x-y\rvert}{1+\lvert x-y\rvert} \\
                                               & \leq \frac{\lvert x-y\rvert}{1}               \\
                                               & = \lvert x-y\rvert                            \\
                                               & = \rho_2(x, y)                                \\
                                               & < \delta \leq \varepsilon
                              \end{align*}
                              On the other hand, suppose that $\rho(x,y)<\delta$. Then
                              \begin{align*}
                                    \rho_2(x, y) & = \lvert x-y\rvert                                                \\
                                                 & = (1+\lvert x-y\rvert)\frac{\lvert x-y\rvert}{1+\lvert x-y\rvert} \\
                                                 & = (1+\lvert x-y\rvert)\rho(x,y)                                   \\
                                                 & < (1+\lvert x-y\rvert)\delta                                      \\
                                                 & = (1+\lvert x-y\rvert)\frac{\varepsilon}{1+\lvert x-y\rvert}      \\
                                                 & = \varepsilon
                              \end{align*}
                        \end{proof}
                  \item Prove that $\rho$ is not uniformly equivalent to
                        $\rho_2$.
                        \begin{proof}
                              Let $\varepsilon>0$ be given. Suppose there exists
                              some $\delta(\varepsilon)$ such that
                              $\rho(x,y)<\delta\implies\rho_2(x,y)<\varepsilon$
                              and $\rho_2(x,y)<\delta\implies\rho(x,y)<\varepsilon$.
                              Suppose $x\neq y$ and $\varepsilon < \lvert x-y\rvert$.
                              Then it follows that, if $\rho(x,y)<\delta$,
                              \begin{align*}
                                    \rho_2(x,y) & = \lvert x-y\rvert                                                \\
                                                & = (1+\lvert x-y\rvert)\frac{\lvert x-y\rvert}{1+\lvert x-y\rvert} \\
                                                & = (1+\lvert x-y\rvert)\rho(x,y)                                   \\
                                                & = \rho(x,y) + \lvert x-y\rvert\rho(x,y)                           \\
                                                & > \rho(x,y) + \varepsilon\rho(x,y)
                              \end{align*}
                              Therefore, $\rho$ and $\rho_2$ are not uniformly equivalent.
                              % TODO: This is not well fleshed out at all and probably isn't correct
                        \end{proof}
            \end{enumerate}
\end{enumerate}

\subsection{The Contraction Mapping Principle}
\begin{enumerate}
      \setcounter{enumi}{3}
      \item Which of the subsets of $\mathbb{R}^2$ are complete metric spaces
            with the Euclidean metric?
            \begin{enumerate}
                  \item $\{(x, y)\in\mathbb{R}^2|x^2+y^2<1 \}$.
                        \begin{proof}[NOPE]
                              Let $\{p_n\} = (\frac{n-1}{n}, 0)$. Because
                              $n$ is always positive, $\frac{n-1}{n}<1$ so clearly
                              $(\frac{n-1}{n})^2+0^2<1$ and $\{p_n\}\in\mathbb{R}^2$.
                              However, $\lim_{n\to\infty}\frac{n-1}{n}=1$ and
                              $(1,0)$ is not in the subset! Contraction!
                        \end{proof}
                  \item $\{(x, y)\in\mathbb{R}^2|x\geq 1\text{ and }y\leq -2\}$.
                        \begin{proof}
                              Let $\{p_n\}$ be a Cauchy sequence in this subset.
                              Then $\{p_n\}$ is Cauchy in $\mathbb{R}^2$ and
                              therefore has a limit $p=(x,y)$ in $\mathbb{R}^2$.
                              If $p_n=(x_n, y_n)$ then we know that $x_n\to x$
                              and $y_n\to y$. Because all $p_n$ are in the subset,
                              we know that $x_n\geq 1$ and $y_n\leq -2$. Therefore,
                              $x\geq 1$ and $y\leq -2$ so the metric space is complete.
                        \end{proof}
                  \item $\{(x, y)\in\mathbb{R}^2|y\in\mathbb{N} \}$.
                        \begin{proof}
                              Because $\mathbb{N}$ is discrete, its Cauchy
                              sequences are all constant. Then for any Cauchy
                              sequence $\{p_n\}$ in this subset with the limit
                              $p=(x,y)$, $p_n=(x_n,y)$. Since $p_n\to p$,
                              we know that $x_n\to x$. All $x\in\mathbb{R}$
                              are a part of this subset, so we know that the
                              metric space is complete.
                        \end{proof}
                  \item $\{(x, y)\in\mathbb{R}^2|f(x,y)=0 \}$, where $f$ is
                        continuous on $\mathbb{R}^2$.
                        \begin{proof}
                              I'm convinced that this is true, but I do not know
                              how to prove it. Picture a 3d function that intersects
                              with the plane $z=0$ all over the place. Since $f$ is
                              continuous there aren't any discontinuities on $z=0$,
                              which means that any subsequence will be able to find
                              a home.
                              % TODO: actually prove this
                        \end{proof}
            \end{enumerate}
      \item Which of the following subsets of $C[a,b]$ are complete metric
            spaces with the metric $\rho_\infty$?
            \begin{enumerate}
                  \item $\{f\in C[a,b]|f(x)>0\text{ for }x\in [a,b]\}$.
                        \begin{proof}[NOPE]
                              Let $f_n(x)=\frac{1}{n}$. It is clear that for all
                              $n\in\mathbb{N}$, $f_n(x)>0$ However,
                              $\lim_{n\to\infty}f_n(x)=f(x)=0$. This is not in
                              the subset. Contraction!
                        \end{proof}
                  \item $\{f\in C[a,b]|f(a)=0\}$.
                        \begin{proof}
                              Let $\{f_n\}$ be a Cauchy sequence in this subset.
                              Then, since $\{f_n\}$ is on $C[a,b]$ and $C[a,b]$
                              is complete, there is a continuous function $f$
                              on $[a,b]$ such that $f_n\to f$ uniformly. For all
                              $n$, $f_n(a)=0$, and this implies that
                              $f(a)=\lim_{n\to\infty}f_n(a)=0$. Therefore, this
                              subset is complete.
                        \end{proof}
                  \item $\{f\in C[a,b]|f(x)=0\text{ for }a<c\leq x\leq d<b\}$.
                        \begin{proof}
                              Let $\{f_n\}$ be a Cauchy sequence in this subset.
                              Then, since $\{f_n\}$ is on $C[a,b]$ and $C[a,b]$
                              is complete, there is a continuous function $f$
                              on $[a,b]$ such that $f_n\to f$ uniformly. We also
                              know that for all $x\in[c,d]$, $f(x)=0$. Since
                              $C[a,b]$ is complete in $\rho_\infty$, and all
                              $f_n$ in the sequence are equivalent to 0 on
                              $[c,d]$, it is clear that $f$ is also in this
                              subset.
                        \end{proof}
                  \item $\{f\in C[a,b]|\lvert f(x)\rvert\leq 2+f(x)^2$
                        for $x\in[a,b]\}$.
                        \begin{proof}
                              Let $\{f_n\}$ be a Cauchy sequence in this subset.
                              Then, since $\{f_n\}$ is on $C[a,b]$ and $C[a,b]$
                              is complete, there is a continuous function $f$
                              on $[a,b]$ such that $f_n\to f$ uniformly.
                              % TODO: I don't know how to do this one at all
                        \end{proof}
            \end{enumerate}
            \setcounter{enumi}{9}
      \item \begin{enumerate}
                  \item Prove that if $\rho$ and $\sigma$ are uniformly
                        equivalent metrics on $\mathcal{M}$, then
                        $(\mathcal{M}, \rho)$ is complete only if
                        $(\mathcal{M}, \sigma)$ is complete.
                        \begin{proof}
                              Suppose $\sigma$ is not a complete metric in
                              $\mathcal{M}$. Then there exists some
                              $\{f_n\}\in(\mathcal{M}, \sigma)$ so that
                              $\lim_{n\to\infty}(f_n)\notin(\mathcal{M}, \sigma)$
                              Because $\sigma$ and $\rho$ are uniformly equivalent,
                              $\lim_{n\to\infty}(f_n)\notin(\mathcal{M}, \rho)$.
                              This cannot be the case, since $(\mathcal{M}, \sigma)$
                              is complete. Therefore, $(\mathcal{M}, \rho)$ is complete only if
                              $(\mathcal{M}, \sigma)$ is complete.
                              %TODO: I don't think this is correct
                        \end{proof}
                  \item Suppose that $\rho$ and $\sigma$ are equivalent metrics
                        on $\mathcal{M}$. Show by example that it is possible
                        that $(\mathcal{M}, \rho)$ is complete but
                        $(\mathcal{M}, \sigma)$ is not complete.
                        \begin{proof}
                              Let $\mathcal{M}=\mathbb{R}^2$, and
                              \begin{align*}
                                    \rho(x,y)   & \equiv \lvert x-y\rvert                             \\
                                    \sigma(x,y) & \equiv \frac{\lvert x-y\rvert}{1+\lvert x-y\rvert}. \\
                              \end{align*}
                              % TODO: This is probably a bad example, I don't know how it would work
                        \end{proof}
            \end{enumerate}
            \setcounter{enumi}{11}
      \item Let $\mathcal{M}$ be the set of continuous functions on $\mathbb{R}$
            which vanish outside a finite interval (the interval may depend on
            the function).
            \begin{enumerate}
                  \item Show that $\mathcal{M}$ is a metric space in the sup
                        norm.
                        \begin{proof}
                              Because all functions in $\mathcal{M}$ vanish
                              outside of a finite interval $(a,b)$, we can model
                              each function in $\mathcal{M}$ as
                              \[
                                    f(x) = \begin{cases}
                                          0    & \textrm{if } x\leq a \\
                                          f(x) & \textrm{if } a<x<b   \\
                                          0    & \textrm{if } b\leq x
                                    \end{cases}.
                              \]
                              We can imagine two cases from here. First imagine
                              that for all $x$, $f(x)\leq 0$. Then
                              $\lVert f(x)\rVert_\infty=0$ because the function
                              vanishes outside of the finite interval. Otherwise,
                              $\lVert f(x)\rVert_\infty=\lVert f(x)\rVert_{(a,b)}$.
                        \end{proof}
                  \item Show that $\mathcal{M}$ is not complete.
                        \begin{proof}
                              Let
                              \begin{align*}
                                    f_n(x)                  & = \begin{cases}
                                          0                    & \textrm{if } x\leq -1 \\
                                          1-x^{\lvert n\rvert} & \textrm{if } -1<x<1   \\
                                          0                    & \textrm{if } 1\leq x
                                    \end{cases}.
                                    \intertext{Then}
                                    \lim_{n\to\infty}f_n(x) & = \begin{cases}
                                          0 & \textrm{if } x\leq -1 \\
                                          1 & \textrm{if } -1<x<1   \\
                                          0 & \textrm{if } 1\leq x
                                    \end{cases},
                              \end{align*}
                              which is not continuous.
                        \end{proof}
                  \item Show that $C_o(\mathbb{R})$, the continuous functions
                        which go to zero at $\infty$, is complete in the sup
                        norm.
                        \begin{proof}[Wait...]
                              But doesn't $2^{-x}$ vanish as $x\to \infty$? That
                              function is not in the sup norm as far as I understand.
                              % TODO: figure this out
                        \end{proof}
                  \item Prove that $\mathcal{M}$ is dense in $C_o(\mathbb{R})$.
                        \begin{proof}
                              I can't do this without (c), which I do not know how to do.
                              % TODO: Learn about dense functions
                        \end{proof}
            \end{enumerate}
\end{enumerate}

\subsection{Normed Linear Spaces}
\begin{enumerate}
      \setcounter{enumi}{7}
      \item Two norms, $\lVert\cdot\rVert_1$ and $\lVert\cdot\rVert_2$, are
            called \textbf{equivalent} if there are positive constants, $c$ and
            $d$, so that
            \[
                  c\lVert v\rVert_1\leq\lVert v\rVert_2\leq d\lVert v\rVert_1
            \]
            for all $v\in V$.
            \begin{enumerate}
                  \item Prove that if $\lVert\cdot\rVert_1$ and
                        $\lVert\cdot\rVert_2$  are equivalent, then $V$ is
                        complete in $\lVert\cdot\rVert_1$ if and only if $V$ is
                        complete in $\lVert\cdot\rVert_2$.
                        \begin{proof}
                              Let us assume on the contrary that $V$ is complete
                              in $\lVert\cdot\rVert_1$ but it is not complete in
                              $\lVert\cdot\rVert_2$. Then there exists some Cauchy
                              sequence $\{a_n\}\in(V,\lVert\cdot\rVert_1)$ such that
                              $\lim_{n\to\infty}\{a_n\}\in(V,\lVert\cdot\rVert_1)$.
                              Let $a$ be $\lim_{n\to\infty}\{a_n\}$.
                              Because $\lVert\cdot\rVert_1$ is equivalent to
                              $\lVert\cdot\rVert_2$, we know that there exists
                              positive constants $c$ and $d$ such that
                              $c\lVert a\rVert_1\leq\lVert a\rVert_2\leq d\lVert a\rVert_1$.
                              Since $c\lVert a\rVert_1$ and $d\lVert a\rVert_1$
                              are both in $V$, it is clear that $\lVert a\rVert_2$
                              is also in $V$. This is a contradiction! Therefore,
                              $\lVert\cdot\rVert_2$ is complete implies that
                              $\lVert\cdot\rVert_1$ is complete.
                        \end{proof}
                  \item Prove that the $\lVert\cdot\rVert_1$ norm and the
                        Euclidean norm $\lVert\cdot\rVert_2$ are equivalent on
                        $\mathbb{R}^n$ by showing that
                        \[
                              \lVert x\rVert_2^2
                              \leq \lVert x\rVert_1^2
                              \leq n\lVert x\rVert_2^2.
                        \]
                        \begin{proof}
                              Let us represent members of $\mathbb{R}^n$ as
                              vectors of the following form:
                              \[
                                    (x_0, x_1, x_2, \ldots, x_{(n-3)}, x_{(n-2)}, x_{(n-1)}).
                              \]
                              Then we define
                              \begin{align*}
                                    \lVert\cdot\rVert_1 & \equiv \sum_{i=0}^{n-1}\lvert x_i\rvert \\
                                    \lVert\cdot\rVert_2 & \equiv \sqrt{\sum_{i=0}^{n-1}(x_i)^2}.
                              \end{align*}
                              The norms are equivalent if there exists some positive
                              scalar $c$ and $d$ such that
                              \[
                                    c\lVert\cdot\rVert_2 \leq \lVert\cdot\rVert_1 \leq d\lVert\cdot\rVert_2.
                              \]
                              Let $c=1$ and $d=\sqrt{n}$. Then it follows that
                              \[
                                    \lVert\cdot\rVert_2^2 \leq \lVert\cdot\rVert_1^2 \leq n\lVert\cdot\rVert_2^2.
                              \]
                              To show that this is true, let us select some arbitrary vector
                              $x\in\mathbb{R}^n$. Then
                              \begin{align*}
                                    \lVert x\rVert_2           & = \sqrt{\sum_{i=0}^{n-1}(x_i)^2}                     \\
                                    \lVert x\rVert_2^2         & = \sum_{i=0}^{n-1}(x_i)^2                            \\
                                                               & \leq \left(\sum_{i=0}^{n-1}\lvert x_i\rvert\right)^2 \\
                                                               & = \lVert x\rVert_1^2                                 \\
                                                               & \leq n\sqrt{\sum_{i=0}^{n-1}(x_i)^2}                 \\
                                                               & = n\lVert x\rVert_2^2                                \\
                                    \\
                                    \therefore\lVert x\rVert_2 & \leq \lVert x\rVert_1\leq \lVert x\rVert_2
                              \end{align*}
                        \end{proof}
                  \item Prove that the sup norm and the $L_1$ norm are not
                        equivalent on $C[a,b]$.
                        \begin{proof}
                              % TODO: this proof
                        \end{proof}
            \end{enumerate}
            \setcounter{enumi}{9}
      \item Let $\{x_i\}_{i=1}^N$ and $\{y_i\}_{i=1}^N$ be real numbers and not
            all zero. Define a quadratic, $p(\lambda)$, by
            \[
                  p(\lambda)=\sum_{i=1}^N(x_i+\lambda y_i)^2.
            \]
            Explain why $p(\lambda)$ has either two complex roots or a double
            real root. Use this fact to prove the \textbf{Cauchy-Schwartz
                  Inequality}.
            \begin{equation}
                  \left\lvert\sum_{i=1}^Nx_iy_i\right\rvert \leq
                  \left(\sum_{i=1}^Nx_i^2\right)^\frac{1}{2}
                  \left(\sum_{i=1}^Ny_i^2\right)^\frac{1}{2}.
            \end{equation}
            Under what circumstances does one get equality?
      \item Let $c_o$ denote the set of sequences, $\{a_j\}$, of real numbers such
            that $a_j\to 0$ as $j\to \infty$. Define
            \[
                  \lVert\{a_j\}\rVert_\infty\equiv\sup_j\lvert a_j\rvert.
            \]
            \begin{enumerate}
                  \item Explain why $c_o$ is a normed linear space with the norm
                        $\lVert\cdot\rVert_\infty$.
                  \item Prove that $c_o$ is complete.
                  \item Show that the set of sequences which are zero after
                        finitely many terms is dense in $c_o$.
                  \item Show that $c_o$ is not dense in $\ell_\infty$.
            \end{enumerate}
            \setcounter{enumi}{13}
      \item For $f\in C^{(2)}[a,b]$, define $\lVert f\rVert
                  \equiv\lVert f\rVert_\infty + \lVert f'\rVert_\infty
                  + \lVert f''\rVert_\infty$.
            \begin{enumerate}
                  \item Explain why $C^{(2)}[a,b]$ is a vector space.
                        \begin{enumerate}
                              \item Adding two twice continuously differentiable
                                    functions results in another twice continuously
                                    differentiable function.
                              \item Function addition is clearly commutative.
                              \item Addition is also associative.
                              \item Let $f(x)=0$ be the 0 vector in $C^{(2)}[a,b]$.
                              \item Let $f'(x)$ = $-f(x)$. Then $f'(x)+f(x)=$ the zero vector.
                              \item A twice differentiable function multiplied by a scalar is still twice differentiable.
                              \item $(\alpha+\beta)f(x) = \alpha f(x) + \beta f(x)$.
                              \item $\alpha(\beta \lVert f\rVert) = (\alpha\beta) \lVert f\rVert$.
                              \item $\alpha (f(x)+g(x)) = \alpha f(x) + \alpha g(x)$.
                        \end{enumerate}
                  \item Prove that $\lVert\cdot\rVert$ is a norm in
                        $C^{(2)}[a,b]$.
                        \begin{enumerate}
                              \item By definition, $\lVert\cdot\rVert_\infty\geq 0$.
                                    Adding three nonnegative numbers results in another
                                    nonnegative number.
                              \item \begin{align*}
                                          \lVert \alpha f\rVert & = \lVert\alpha f\rVert_\infty + \lVert\alpha f'\rVert_\infty + \lVert\alpha f''\rVert_\infty                                     \\
                                                                & = \lvert\alpha\rvert\lVert f\rVert_\infty + \lvert\alpha\rvert\lVert f'\rVert_\infty + \lvert\alpha\rvert\lVert f''\rVert_\infty \\
                                                                & = \lvert\alpha\rvert\lVert f\rVert
                                    \end{align*}
                              \item \begin{align*}
                                          \lVert f+g\rVert & = \lVert f+g\rVert_\infty + \lVert f'+g'\rVert_\infty + \lVert f''+g''\rVert_\infty                                                                   \\
                                                           & \leq \lVert f\rVert_\infty+\lVert g\rVert_\infty + \lVert f'\rVert_\infty+\lVert g'\rVert_\infty + \lVert f''\rVert_\infty+\lVert g''\rVert_\infty    \\
                                                           & = \lVert f\rVert_\infty + \lVert f'\rVert_\infty + \lVert f''\rVert_\infty + \lVert g\rVert_\infty + \lVert g'\rVert_\infty + \lVert g''\rVert_\infty \\
                                                           & = \lVert f\rVert + \lVert g\rVert
                                    \end{align*}
                        \end{enumerate}
                  \item Prove that $C^{(2)}[a,b]$ is complete in the norm
                        $\lVert\cdot\rVert$.
                        \begin{proof}
                              Let $\{f_n\}$ be a cauchy sequence in $C^{(2)}[a,b]$. Then,
                              given $\varepsilon>0$, there exists some $N$ so that
                              for all $m,n>N$,
                              \[
                                    \lVert f_m-f_n\rVert < \varepsilon.
                              \]
                              We know that
                              \begin{align*}
                                    \lVert f_m-f_n\rVert & = \lVert f_m - f_n\rVert_\infty + \lVert f_m' - f_n'\rVert_\infty + \lVert f_m'' - f_n''\rVert_\infty                                                  \\
                                                         & \geq \lVert f_m\rVert_\infty - \lVert f_n\rVert_\infty + f_m'\rVert_\infty - \lVert f_n'\rVert_\infty + f_m''\rVert_\infty - \lVert f_n''\rVert_\infty \\
                                                         & = \lVert f_m\rVert - \lVert f_n\rVert
                              \end{align*}
                              So it follows that
                              \[
                                    \lVert f_m\rVert - \lVert f_n\rVert \leq \lVert f_m-f_n\rVert < \varepsilon
                                    \implies \lVert f_m\rVert - \lVert f_n\rVert< \varepsilon.
                              \]
                              Thus, $\{f_n\}$ converges to a real number.
                        \end{proof}
                  \item Prove that $\frac{d^2}{dx^2}$ is a linear
                        transformation from $C^{(2)}[a,b]$ to $C[a,b]$.
                        \begin{proof}
                              Let $f, g\in C^{(2)}[a,b]$, and $\alpha, \beta\in\mathbb{R}$. Then
                              \begin{align*}
                                    \frac{d^2}{dx^2}(\alpha f+\beta g) & = \frac{d^2}{dx^2}(\alpha f) + \frac{d^2}{dx^2}(\beta g) \\
                                                                       & = \alpha\frac{d^2}{dx^2}(f) + \beta\frac{d^2}{dx^2}(g).  \\
                                                                       & = \alpha f'' + \beta g''
                              \end{align*}
                              Since $f$ and $g$ are both members of $C^{(2)}[a,b]$, $f''$ and
                              $g''$ must be continuous functions within the same range. Therefore,
                              $f'',g''\in C[a,b]$ and thus $\alpha f'' + \beta g''\in C[a,b]$.
                              This tells us that $\frac{d^2}{dx^2}$ is a linear transformation.
                        \end{proof}
                  \item Prove that $\{f\in C^{(2)}[a,b]|f''(x)\equiv 0\}$
                        is a vector space. It is called the \textbf{kernel} of
                        $\frac{d^2}{dx^2}$.
                        \begin{enumerate}
                              \item Let $f$ and $g$ be twice continuously differentiable functions
                                    Such that $f''=g''=0$. Then $(f+g)''=0$.
                              \item Function addition is commutative.
                              \item Function addition is also associative.
                              \item Let $f(x)=0$ be the 0 vector in $C^{(2)}[a,b]$.
                              \item Let $f'(x)$ = $-f(x)$. Then $f'(x)+f(x)=$ the zero vector.
                              \item A twice differentiable function multiplied by a scalar is still twice differentiable,
                                    and since the second derivative is 0, multiplying by a scalar will not change it.
                              \item $(\alpha+\beta)f(x) = \alpha f(x) + \beta f(x)$.
                              \item $\alpha(\beta \lVert f\rVert) = (\alpha\beta) \lVert f\rVert$.
                              \item $\alpha (f(x)+g(x)) = \alpha f(x) + \alpha g(x)$.
                        \end{enumerate}
                  \item Identify the functions in the kernel.
                        \medbreak
                        All linear functions are in the kernel.
                  \item Is the linear transformation $\frac{d^2}{dx^2}$
                        one-to-one?
                        \medbreak
                        No, derivatives are lossy. In fact, all functions in the kernel map to
                        the same value undr this linear transformation.
            \end{enumerate}
\end{enumerate}

\subsection*{Projects}
\begin{enumerate}
      \item The purpose of this project is to prove that if $f$ is a continuous
            function on $[0,1]$, then $\lVert f\rVert_p\to\lVert f\rVert_\infty$
            as $p\to \infty$.
            \begin{enumerate}
                  \item Show that for each $p$,
                        $\lVert f\rVert_p\leq\lVert f\rVert_\infty$.
                        \begin{align*}
                              \lVert f\rVert_p & = \sqrt[p]{\int_0^1f^pdx}                     \\
                                               & \leq \sqrt[p]{\int_0^1\sup(f)^pdx}            \\
                                               & = \sqrt[p]{\left. x\sup(f)^p\right\rvert_0^1} \\
                                               & = \sqrt[p]{(1-0)\sup(f)^p}                    \\
                                               & = \sqrt[p]{\sup(f)^p}                         \\
                                               & = \sup(f)                                     \\
                                               & = \lVert f\rVert_\infty
                        \end{align*}
                  \item Explain why $\lvert f(x)\rvert$ is continuous on
                        $[0,1]$. Let $x_o$ be the point where
                        $\lvert f(x)\rvert$ achieves its maximum. Explain why
                        $\lvert f(x_o)\rvert=\lVert f\rVert_\infty$.
                        \medbreak
                        If a function is continuous on $[0,1]$ then its absolute
                        value is also continuous. By definition, the maximum value
                        of the absolute value of the sup norm is the function.
                  \item Assume that $x_o$ is not one of the endpoints and let
                        $\varepsilon>0$ be given. Explain why you can choose
                        a $\mu>0$ so that
                        $\lvert f(x)\rvert\geq\lVert f\rVert_\infty - \varepsilon$ for all
                        $x\in[x_o-\mu,x_o+\mu]$.
                        \medbreak
                        Because $f$ is contained in a finite interval,
                        $\lVert f\rVert_\infty=f(x_o)$. $f$ is also continuous,
                        so there must exist some $\mu$ such that
                        $x_o\pm\mu>\lVert f\rVert_\infty \geq f(x_o)-\varepsilon$.
                  \item Prove that $\int_0^1\lvert f(x)\rvert^pdx \geq
                              2\mu(\lVert f\rVert_\infty-\varepsilon)^p$ for
                        all $p$.
                        \begin{proof}
                              \begin{align*}
                                    \int_0^1\lvert f(x)\rvert^pdx & =
                                    \int_0^1\lvert f(x)\rvert^pdx & \geq 2\mu(\lVert f\rVert_\infty-\varepsilon)^p         \\
                                    \lVert f\rVert_p^p            & \geq 2\mu(\lVert f\rVert_\infty-\varepsilon)^p         \\
                                    \lVert f\rVert_p              & \geq \sqrt[p]{2\mu}(\lVert f\rVert_\infty-\varepsilon) \\
                              \end{align*}
                              % TODO: Sarosh said it was easy but I can't figure it out
                        \end{proof}
                  \item Show that for $p$ large enough, $\lVert f\rVert_\infty
                              \geq \lVert f\rVert_p \geq \lVert f\rVert_\infty
                              -2\varepsilon$ and conclude that $\lim_{p\to\infty}
                              \lVert f\rVert_p = \lVert f\rVert_\infty$.
            \end{enumerate}
            \setcounter{enumi}{3}
      \item The purpose of this project is to solve the differential equation
            satisfied by the extremal function for the Brachistochrone problem.
            The extremal $y(x)$ satisfies $y(x)(1+(y'(x))^2)=C_1$ for some
            constant $C_1$.
            \begin{enumerate}
                  \item Explain why $C_1<0$ and why $y'(x)$ must blow up as
                        $x\searrow  0$. What does it mean geometrically that
                        $y'(x)$ blows up?
                        \medbreak
                        Geometrically, $y'(x)$ blowing up means that the
                        rate of change increases drastically.
                  \item Prove that $\frac{dx}{dy}=\sqrt{\frac{y}{C_1-y}}$.
                  \item We introduce a new variable $\theta$ as a parameter and
                        try to find $x$ and $y$ in terms of $\theta$. Let $y$
                        and $\theta$ be related by the equation
                        $y=C_1(\sin\theta)^2$. Use the chain rule to prove that
                        \[
                              \frac{dx}{d\theta}=C_1(1-\cos 2\theta).
                        \]
                  \item Find $x(\theta)$ in terms of $C_1$ and a new
                        $C_2$.
                  \item Show that the constants $C_1$ and $C_2$ can be chosen
                        so that the curve $(x(\theta),y(\theta))$ passes through
                        the points $(0, 0)$ and $(x_1, y_1)$.
                  \item Generate the graph of the curve $(x(\theta),y(\theta))$.
                        Why do you think that the curve which gives the shortest
                        time of descent is so steep near the origin?
            \end{enumerate}
\end{enumerate}

\end{document}